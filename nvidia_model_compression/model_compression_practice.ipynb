{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/nvidia_header.png\" style=\"margin-left: -30px; width: 300px; float: left;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NVIDIA ëª¨ë¸ ì••ì¶• ì¢…í•© ì‹¤ìŠµ\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ ëª¨ë¸ ì••ì¶•ì˜ ì „ì²´ ì›Œí¬í”Œë¡œìš°ë¥¼ ì‹¤ìŠµí•©ë‹ˆë‹¤.\n",
    "\n",
    "## ëª©ì°¨\n",
    "1. [í™˜ê²½ ì„¤ì • ë° ê¸°ì¤€ ëª¨ë¸](#1.-í™˜ê²½-ì„¤ì •-ë°-ê¸°ì¤€-ëª¨ë¸)\n",
    "2. [ì–‘ìí™” (Quantization)](#2.-ì–‘ìí™”-Quantization)\n",
    "3. [í”„ë£¨ë‹ (Pruning)](#3.-í”„ë£¨ë‹-Pruning)\n",
    "4. [ì§€ì‹ ì¦ë¥˜ (Knowledge Distillation)](#4.-ì§€ì‹-ì¦ë¥˜-Knowledge-Distillation)\n",
    "5. [ì¶”ë¡  ë°°í¬ ë° ì„±ëŠ¥ ì¸¡ì •](#5.-ì¶”ë¡ -ë°°í¬-ë°-ì„±ëŠ¥-ì¸¡ì •)\n",
    "6. [ì¢…í•© ë¹„êµ ë° ë¶„ì„](#6.-ì¢…í•©-ë¹„êµ-ë°-ë¶„ì„)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. í™˜ê²½ ì„¤ì • ë° ê¸°ì¤€ ëª¨ë¸\n",
    "\n",
    "### 1.1 í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import json\n",
    "import logging\n",
    "from typing import List, Dict\n",
    "\n",
    "# NeMo & TensorRT\n",
    "from nemo.export.tensorrt_llm import TensorRTLLM\n",
    "from nemo.deploy import DeployPyTriton\n",
    "from nemo.deploy.nlp import NemoQueryLLM\n",
    "\n",
    "# ì‹œê°í™”\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# ë¡œê¹… ì„¤ì •\n",
    "logging.getLogger(\"TRT-LLM\").setLevel(logging.ERROR)\n",
    "\n",
    "print(\"âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ import ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 ì „ì—­ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ëª¨ë¸ ê²½ë¡œ\n",
    "HF_CHECKPOINT = \"Llama-3.2-3B\"\n",
    "NEMO_CHECKPOINT = \"Llama-3.2-3B.nemo\"\n",
    "\n",
    "# ë””ë ‰í† ë¦¬ ì„¤ì •\n",
    "os.makedirs(\"qnemo\", exist_ok=True)\n",
    "os.makedirs(\"engines\", exist_ok=True)\n",
    "os.makedirs(\"experiments\", exist_ok=True)\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "# ì—”ì§„ êµ¬ì„±\n",
    "MAX_BS = 4\n",
    "MAX_INPUT_LEN = 2048\n",
    "MAX_OUTPUT_LEN = 2048\n",
    "\n",
    "print(\"âœ… ì „ì—­ ì„¤ì • ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 NeMo ì²´í¬í¬ì¸íŠ¸ ì¤€ë¹„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NeMo ì²´í¬í¬ì¸íŠ¸ ì´ë™\n",
    "src_file = os.path.join(\"/dli/task/Llama-3.2-3B\", \"Llama-3.2-3B.nemo\")\n",
    "dst_file = os.path.join(\"/dli/task\", \"Llama-3.2-3B.nemo\")\n",
    "\n",
    "if os.path.exists(dst_file):\n",
    "    print(f\"âœ… NeMo ì²´í¬í¬ì¸íŠ¸ ì¤€ë¹„ ì™„ë£Œ: {dst_file}\")\n",
    "else:\n",
    "    print(f\"â³ {src_file} ë‹¤ìš´ë¡œë“œ ëŒ€ê¸° ì¤‘...\")\n",
    "    while not os.path.exists(src_file):\n",
    "        time.sleep(10)\n",
    "        print(f\"ì—¬ì „íˆ ëŒ€ê¸° ì¤‘...\")\n",
    "    \n",
    "    shutil.move(src_file, dst_file)\n",
    "    print(f\"âœ… ì²´í¬í¬ì¸íŠ¸ ì´ë™ ì™„ë£Œ: {dst_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 ê¸°ì¤€ ëª¨ë¸ (BF16) ì—”ì§„ ë¹Œë“œ\n",
    "\n",
    "ë¨¼ì € BF16 ì •ë°€ë„ë¡œ ê¸°ì¤€ ëª¨ë¸ì˜ TensorRT-LLM ì—”ì§„ì„ ë¹Œë“œí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "NEMO_CHECKPOINT=\"Llama-3.2-3B.nemo\"\n",
    "QNEMO_PATH=\"qnemo/llama-3.2-3b/bf16\"\n",
    "\n",
    "python3 /opt/NeMo/examples/nlp/language_modeling/megatron_gpt_ptq.py \\\n",
    "    model.restore_from_path=$NEMO_CHECKPOINT \\\n",
    "    trainer.precision=bf16 \\\n",
    "    quantization.algorithm=null \\\n",
    "    export.decoder_type=llama \\\n",
    "    export.inference_tensor_parallel=1 \\\n",
    "    export.save_path=$QNEMO_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ENGINE_DIR=\"engines/llama-3.2-3b/bf16\"\n",
    "QNEMO_PATH=\"qnemo/llama-3.2-3b/bf16\"\n",
    "\n",
    "trtllm-build --checkpoint_dir $QNEMO_PATH \\\n",
    "    --gemm_plugin auto \\\n",
    "    --output_dir $ENGINE_DIR \\\n",
    "    --max_batch_size 4 \\\n",
    "    --max_input_len 2048 \\\n",
    "    --max_seq_len 2048 \\\n",
    "    --gather_context_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 ê¸°ì¤€ ëª¨ë¸ MMLU í‰ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "HF_CHECKPOINT=\"Llama-3.2-3B\"\n",
    "ENGINE_DIR=\"engines/llama-3.2-3b/bf16\"\n",
    "\n",
    "python3 /workspace/tensorrtllm_backend/tensorrt_llm/examples/mmlu.py \\\n",
    "    --data_dir \"/workspace/data/mmlu\" \\\n",
    "    --model_dir $HF_CHECKPOINT \\\n",
    "    --engine_dir $ENGINE_DIR \\\n",
    "    --test_trt_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ì˜ˆìƒ ê²°ê³¼:** MMLU ì •í™•ë„ ì•½ **75%**\n",
    "\n",
    "ì´ ê°’ì´ ìš°ë¦¬ì˜ ê¸°ì¤€ì„ ì´ ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. ì–‘ìí™” (Quantization)\n",
    "\n",
    "### 2.1 ì–‘ìí™” ê°œë…\n",
    "\n",
    "ì–‘ìí™”ëŠ” ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ì™€ í™œì„±í™”ë¥¼ ë” ë‚®ì€ ë¹„íŠ¸ í‘œí˜„ìœ¼ë¡œ ë³€í™˜í•˜ì—¬:\n",
    "- ëª¨ë¸ í¬ê¸° ê°ì†Œ\n",
    "- ë©”ëª¨ë¦¬ ëŒ€ì—­í­ ê°ì†Œ\n",
    "- ì¶”ë¡  ì†ë„ í–¥ìƒ\n",
    "\n",
    "ì´ ì„¹ì…˜ì—ì„œëŠ” **FP8 ì–‘ìí™”**ë¥¼ ì ìš©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 FP8 QNeMo ì²´í¬í¬ì¸íŠ¸ ìƒì„±\n",
    "\n",
    "FP8 ì–‘ìí™”ëŠ” ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰ë©ë‹ˆë‹¤:\n",
    "1. ìº˜ë¦¬ë¸Œë ˆì´ì…˜ (í™œì„±í™” í‰í™œí™”)\n",
    "2. ê°€ì¤‘ì¹˜ ì–‘ìí™”\n",
    "3. QNeMo ì²´í¬í¬ì¸íŠ¸ ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "NEMO_CHECKPOINT=\"Llama-3.2-3B.nemo\"\n",
    "QNEMO_PATH=\"qnemo/llama-3.2-3b/fp8\"\n",
    "\n",
    "python3 /opt/NeMo/examples/nlp/language_modeling/megatron_gpt_ptq.py \\\n",
    "    model.restore_from_path=$NEMO_CHECKPOINT \\\n",
    "    trainer.precision=bf16 \\\n",
    "    quantization.algorithm=fp8 \\\n",
    "    export.decoder_type=llama \\\n",
    "    export.inference_tensor_parallel=1 \\\n",
    "    export.save_path=$QNEMO_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ê´€ì°° ì‚¬í•­:**\n",
    "- ë¡œê·¸ì—ì„œ `Calibration` í•­ëª©ì„ í™•ì¸í•˜ì„¸ìš”\n",
    "- `TensorQuantizer(amax=[...])` - ì–‘ìí™” ë²”ìœ„\n",
    "- ê°€ì¤‘ì¹˜ì™€ í™œì„±í™”ì˜ ì–‘ìí™” ì°¨ì´"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 FP8 TensorRT ì—”ì§„ ë¹Œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "ENGINE_DIR=\"engines/llama-3.2-3b/fp8\"\n",
    "QNEMO_PATH=\"qnemo/llama-3.2-3b/fp8\"\n",
    "\n",
    "trtllm-build --checkpoint_dir $QNEMO_PATH \\\n",
    "    --gemm_plugin auto \\\n",
    "    --output_dir $ENGINE_DIR \\\n",
    "    --max_batch_size 4 \\\n",
    "    --max_input_len 2048 \\\n",
    "    --max_seq_len 2048 \\\n",
    "    --gather_context_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 ì–‘ìí™”ëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "HF_CHECKPOINT=\"Llama-3.2-3B\"\n",
    "ENGINE_DIR=\"engines/llama-3.2-3b/fp8\"\n",
    "\n",
    "python3 /workspace/tensorrtllm_backend/tensorrt_llm/examples/run.py \\\n",
    "    --engine_dir $ENGINE_DIR \\\n",
    "    --tokenizer_dir $HF_CHECKPOINT \\\n",
    "    --max_output_len 25 \\\n",
    "    --input_text \"What is a graphics processing unit?\" \\\n",
    "    --run_profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 FP8 ëª¨ë¸ MMLU í‰ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "HF_CHECKPOINT=\"Llama-3.2-3B\"\n",
    "ENGINE_DIR=\"engines/llama-3.2-3b/fp8\"\n",
    "\n",
    "python3 /workspace/tensorrtllm_backend/tensorrt_llm/examples/mmlu.py \\\n",
    "    --data_dir \"/workspace/data/mmlu\" \\\n",
    "    --model_dir $HF_CHECKPOINT \\\n",
    "    --engine_dir $ENGINE_DIR \\\n",
    "    --test_trt_llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ì˜ˆìƒ ê²°ê³¼:** MMLU ì •í™•ë„ ì•½ **73%**\n",
    "\n",
    "ë¶„ì„:\n",
    "- ê¸°ì¤€ ëŒ€ë¹„ 2%p ì†ì‹¤\n",
    "- ëª¨ë¸ í¬ê¸° 50% ê°ì†Œ\n",
    "- ì¶”ë¡  ì†ë„ ì•½ 2ë°° í–¥ìƒ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. í”„ë£¨ë‹ (Pruning)\n",
    "\n",
    "### 3.1 í”„ë£¨ë‹ ê°œë…\n",
    "\n",
    "í”„ë£¨ë‹ì€ ì‹ ê²½ë§ì—ì„œ ë¶ˆí•„ìš”í•œ ë¶€ë¶„ì„ ì œê±°í•©ë‹ˆë‹¤:\n",
    "- **ê¹Šì´ í”„ë£¨ë‹**: ì „ì²´ ë ˆì´ì–´ ì œê±°\n",
    "- **í­ í”„ë£¨ë‹**: ê°œë³„ ë‰´ëŸ°/í—¤ë“œ ì œê±°\n",
    "\n",
    "ì´ ì„¹ì…˜ì—ì„œëŠ” **ê¹Šì´ í”„ë£¨ë‹**ì„ ìˆ˜í–‰í•˜ì—¬ ë§ˆì§€ë§‰ 25% ë ˆì´ì–´ë¥¼ ì œê±°í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 ê¹Šì´ í”„ë£¨ë‹ ìˆ˜í–‰\n",
    "\n",
    "Llama-3.2-3BëŠ” 28ê°œì˜ ë ˆì´ì–´ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤.\n",
    "25% ì œê±° â†’ ë§ˆì§€ë§‰ 7ê°œ ë ˆì´ì–´ (21-27) ì‚­ì œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "python /opt/NeMo/examples/nlp/language_modeling/megatron_gpt_drop_layers.py \\\n",
    "    --path_to_nemo \"Llama-3.2-3B.nemo\" \\\n",
    "    --path_to_save \"Llama-3.2-3B-pruned.nemo\" \\\n",
    "    --tensor_model_parallel_size 1 \\\n",
    "    --pipeline_model_parallel_size 1 \\\n",
    "    --gpus_per_node 1 \\\n",
    "    --drop_layers 21 22 23 24 25 26 27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 í”„ë£¨ë‹ëœ ëª¨ë¸ í‰ê°€\n",
    "\n",
    "lm-evaluation-harnessë¥¼ ì‚¬ìš©í•˜ì—¬ í‰ê°€í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "lm_eval --model nemo_lm \\\n",
    "    --model_args path=\"Llama-3.2-3B-pruned.nemo\" \\\n",
    "    --tasks mmlu_high_school_world_history,mmlu_high_school_government_and_politics,mmlu_world_religions,mmlu_sociology \\\n",
    "    --num_fewshot 3 \\\n",
    "    --batch_size 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ì˜ˆìƒ ê²°ê³¼:** MMLU ì •í™•ë„ ì•½ **65%**\n",
    "\n",
    "ë¶„ì„:\n",
    "- ê¸°ì¤€ ëŒ€ë¹„ 10%p ì†ì‹¤\n",
    "- ëª¨ë¸ í¬ê¸° 25% ê°ì†Œ\n",
    "- âš ï¸ ì„±ëŠ¥ ì €í•˜ - ì¦ë¥˜ í•„ìš”!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. ì§€ì‹ ì¦ë¥˜ (Knowledge Distillation)\n",
    "\n",
    "### 4.1 ì¦ë¥˜ ê°œë…\n",
    "\n",
    "ì§€ì‹ ì¦ë¥˜ëŠ” í° ëª¨ë¸(TEACHER)ì˜ ì§€ì‹ì„ ì‘ì€ ëª¨ë¸(STUDENT)ì—ê²Œ ì „ë‹¬í•©ë‹ˆë‹¤:\n",
    "- **TEACHER**: Llama-3.2-3B.nemo (ì›ë³¸)\n",
    "- **STUDENT**: Llama-3.2-3B-pruned.nemo (í”„ë£¨ë‹ë¨)\n",
    "- **ë°ì´í„°**: WikiText-103-v1\n",
    "\n",
    "ëª©í‘œ: í”„ë£¨ë‹ìœ¼ë¡œ ì†ì‹¤ëœ ì„±ëŠ¥ ë³µêµ¬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 TensorBoard ì‹œì‘\n",
    "\n",
    "ì¦ë¥˜ ì§„í–‰ ìƒí™©ì„ ëª¨ë‹ˆí„°ë§í•˜ê¸° ìœ„í•´ TensorBoardë¥¼ ì‹œì‘í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%js\n",
    "const href = window.location.hostname;\n",
    "let a = document.createElement(\"a\");\n",
    "a.textContent = \"Open TensorBoard\";\n",
    "a.href = \"http://\" + href + \":8886\";\n",
    "a.style.cssText = \"text-decoration: none; color: white; font-size: 20px; font-weight: bold; padding: 15px 25px; background-color: #76B900; border-radius: 8px; display: inline-block; box-shadow: 2px 2px 10px rgba(0, 0, 0, 0.2);\";\n",
    "a.target = \"_blank\";\n",
    "a.onmouseover = () => a.style.backgroundColor = \"#5a9300\";\n",
    "a.onmouseout = () => a.style.backgroundColor = \"#76B900\";\n",
    "element.appendChild(a);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 ì§€ì‹ ì¦ë¥˜ ì‹¤í–‰\n",
    "\n",
    "â±ï¸ **ì˜ˆìƒ ì†Œìš” ì‹œê°„**: ì•½ 20-30ë¶„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export CUDA_DEVICE_MAX_CONNECTIONS=1\n",
    "\n",
    "# ì„¤ì • ë³€ìˆ˜\n",
    "TENSOR_PARALLEL_SIZE=1\n",
    "EXPERIMENT_DIR=\"./experiments/\"\n",
    "EXPERIMENT_NAME=\"llama_distill_depth_pruned_student\"\n",
    "\n",
    "# ëª¨ë¸ ê²½ë¡œ\n",
    "TEACHER=\"Llama-3.2-3B.nemo\"\n",
    "STUDENT=\"Llama-3.2-3B-pruned.nemo\"\n",
    "FINAL_MODEL_PATH=\"Llama-3.2-3B-distilled.nemo\"\n",
    "\n",
    "# ë°ì´í„° ê²½ë¡œ\n",
    "DATA_TRAIN='./wikidata/wikitext_tokenized_train_text_document'\n",
    "DATA_VAL='./wikidata/wikitext_tokenized_val_text_document'\n",
    "DATA_TEST='./wikidata/wikitext_tokenized_test_text_document'\n",
    "\n",
    "# í† í¬ë‚˜ì´ì €\n",
    "TOK='Llama-3.2-3B'\n",
    "\n",
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
    "STEPS=30\n",
    "GLOBAL_BATCH_SIZE=64\n",
    "MICRO_BATCH_SIZE=1\n",
    "LR=7e-5\n",
    "MIN_LR=1e-5\n",
    "WARMUP_STEPS=2\n",
    "\n",
    "# ì¦ë¥˜ ì‹¤í–‰\n",
    "torchrun --nproc-per-node=${TENSOR_PARALLEL_SIZE} \\\n",
    "    /opt/NeMo/examples/nlp/language_modeling/megatron_gpt_distillation.py \\\n",
    "    name=${EXPERIMENT_NAME} \\\n",
    "    exp_manager.exp_dir=${EXPERIMENT_DIR} \\\n",
    "    exp_manager.checkpoint_callback_params.save_top_k=1 \\\n",
    "    trainer.max_steps=${STEPS} \\\n",
    "    trainer.log_every_n_steps=1 \\\n",
    "    trainer.val_check_interval=50 \\\n",
    "    trainer.limit_val_batches=1 \\\n",
    "    trainer.precision=bf16 \\\n",
    "    trainer.devices=${TENSOR_PARALLEL_SIZE} \\\n",
    "    trainer.num_nodes=1 \\\n",
    "    \"model.data.data_prefix={train:[1.0,$DATA_TRAIN],validation:[$DATA_VAL],test:[$DATA_TEST]}\" \\\n",
    "    model.restore_from_path=${STUDENT} \\\n",
    "    model.kd_teacher_restore_from_path=${TEACHER} \\\n",
    "    model.nemo_path=${FINAL_MODEL_PATH} \\\n",
    "    model.tokenizer.type=${TOK} \\\n",
    "    model.tensor_model_parallel_size=${TENSOR_PARALLEL_SIZE} \\\n",
    "    model.micro_batch_size=${MICRO_BATCH_SIZE} \\\n",
    "    model.global_batch_size=${GLOBAL_BATCH_SIZE} \\\n",
    "    model.optim.lr=${LR} \\\n",
    "    model.optim.sched.min_lr=${MIN_LR} \\\n",
    "    model.optim.sched.warmup_steps=${WARMUP_STEPS} \\\n",
    "    +exp_manager.create_tensorboard_logger=True \\\n",
    "    model.data.seq_length=3072"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TensorBoardì—ì„œ í™•ì¸í•  ì‚¬í•­:**\n",
    "- ì†ì‹¤(Loss) ê°ì†Œ ì¶”ì„¸\n",
    "- KL Divergence ìˆ˜ë ´\n",
    "- í•™ìŠµë¥ (Learning Rate) ìŠ¤ì¼€ì¤„"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 ì¦ë¥˜ëœ ëª¨ë¸ í‰ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "lm_eval --model nemo_lm \\\n",
    "    --model_args path=\"./Llama-3.2-3B-distilled.nemo\" \\\n",
    "    --tasks mmlu_high_school_world_history,mmlu_high_school_government_and_politics,mmlu_world_religions,mmlu_sociology \\\n",
    "    --num_fewshot 3 \\\n",
    "    --batch_size 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ì˜ˆìƒ ê²°ê³¼:** MMLU ì •í™•ë„ ì•½ **72%**\n",
    "\n",
    "ë¶„ì„:\n",
    "- í”„ë£¨ë‹ ëŒ€ë¹„ 7%p í–¥ìƒ!\n",
    "- ê¸°ì¤€ ëŒ€ë¹„ 3%p ì†ì‹¤ë§Œ\n",
    "- í¬ê¸°ëŠ” 75%ë¡œ ìœ ì§€\n",
    "- âœ… ì¦ë¥˜ ì„±ê³µ!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. ì¶”ë¡  ë°°í¬ ë° ì„±ëŠ¥ ì¸¡ì •\n",
    "\n",
    "### 5.1 ì¦ë¥˜ëœ ëª¨ë¸ì˜ TensorRT ì—”ì§„ ë¹Œë“œ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.1 QNeMo ì²´í¬í¬ì¸íŠ¸ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "NEMO_CHECKPOINT=\"Llama-3.2-3B-distilled.nemo\"\n",
    "QNEMO_PATH=\"qnemo/llama-3.2-3b-distilled/bf16\"\n",
    "\n",
    "python3 /opt/NeMo/examples/nlp/language_modeling/megatron_gpt_ptq.py \\\n",
    "    model.restore_from_path=$NEMO_CHECKPOINT \\\n",
    "    trainer.precision=bf16 \\\n",
    "    quantization.algorithm=null \\\n",
    "    export.decoder_type=llama \\\n",
    "    export.inference_tensor_parallel=1 \\\n",
    "    export.save_path=$QNEMO_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.2 TensorRT ì—”ì§„ ë¹Œë“œ (Python API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.export.tensorrt_llm import TensorRTLLM\n",
    "\n",
    "trt_llm_exporter = TensorRTLLM(model_dir=\"engines/llama-3.2-3b-distilled/bf16\")\n",
    "trt_llm_exporter.export(\n",
    "    nemo_checkpoint_path=\"qnemo/llama-3.2-3b-distilled/bf16\",\n",
    "    model_type=\"llama\",\n",
    "    tensor_parallelism_size=1,\n",
    ")\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì¶”ë¡ \n",
    "test_output = trt_llm_exporter.forward([\"What is a graphics processing unit?\"])\n",
    "print(\"í…ŒìŠ¤íŠ¸ ì¶œë ¥:\", test_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Triton Inference Server ë°°í¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.deploy import DeployPyTriton\n",
    "\n",
    "# Triton ì„œë²„ ì‹œì‘\n",
    "nm = DeployPyTriton(\n",
    "    model=trt_llm_exporter,\n",
    "    triton_model_name=\"llama\",\n",
    "    http_port=8000\n",
    ")\n",
    "\n",
    "nm.deploy()\n",
    "nm.run()\n",
    "\n",
    "print(\"âœ… Triton ì„œë²„ ì‹œì‘ ì™„ë£Œ: http://localhost:8000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 ì¶”ë¡  í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nemo.deploy.nlp import NemoQueryLLM\n",
    "\n",
    "nq = NemoQueryLLM(url=\"localhost\", model_name=\"llama\")\n",
    "prompts = [\"What is a graphics processing unit?\"]\n",
    "output = nq.query_llm(prompts=prompts, max_output_len=50)\n",
    "\n",
    "print(\"í”„ë¡¬í”„íŠ¸:\", prompts[0])\n",
    "print(\"\\nì¶œë ¥:\", output[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 ì„±ëŠ¥ ì¸¡ì •\n",
    "\n",
    "#### ì§€ì—° ì‹œê°„ (Latency) ë° ì²˜ë¦¬ëŸ‰ (Throughput) ì¸¡ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# ì„¤ì •\n",
    "num_requests = 5\n",
    "batch_size = 4\n",
    "input_length = 400\n",
    "output_length = 100\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
    "prompts = [\"0\" * input_length] * batch_size\n",
    "\n",
    "# ì„±ëŠ¥ ì¸¡ì •\n",
    "start_time = time.time()\n",
    "for _ in range(num_requests):\n",
    "    output = nq.query_llm(prompts=prompts, max_output_len=output_length)\n",
    "end_time = time.time()\n",
    "\n",
    "# ê³„ì‚°\n",
    "total_time = end_time - start_time\n",
    "throughput = num_requests * batch_size / total_time\n",
    "latency = total_time / num_requests\n",
    "\n",
    "print(f\"ğŸš€ ì²˜ë¦¬ëŸ‰: {throughput:.2f} requests/second\")\n",
    "print(f\"â±ï¸  í‰ê·  ì§€ì—° ì‹œê°„: {latency:.2f} seconds\")\n",
    "print(f\"ğŸ“Š ì´ ìš”ì²­ ìˆ˜: {num_requests * batch_size}\")\n",
    "print(f\"â° ì´ ì†Œìš” ì‹œê°„: {total_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Triton ì„œë²„ ì¤‘ì§€\n",
    "\n",
    "ë‹¤ë¥¸ ëª¨ë¸ì„ í…ŒìŠ¤íŠ¸í•˜ê¸° ì „ì— ì„œë²„ë¥¼ ì¤‘ì§€í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì„œë²„ ì¤‘ì§€ (í•„ìš”í•œ ê²½ìš°)\n",
    "nm.stop()\n",
    "print(\"âœ… Triton ì„œë²„ ì¤‘ì§€ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. ì¢…í•© ë¹„êµ ë° ë¶„ì„\n",
    "\n",
    "### 6.1 ì„±ëŠ¥ ê²°ê³¼ ì •ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²°ê³¼ ë°ì´í„° (ì‹¤ì œ ì¸¡ì •ê°’ìœ¼ë¡œ ì—…ë°ì´íŠ¸)\n",
    "results = {\n",
    "    'ëª¨ë¸': [\n",
    "        'BF16 ê¸°ì¤€',\n",
    "        'FP8 ì–‘ìí™”',\n",
    "        'BF16 í”„ë£¨ë‹',\n",
    "        'BF16 ì¦ë¥˜',\n",
    "        'FP8 ì¦ë¥˜'\n",
    "    ],\n",
    "    'MMLU (%)': [75, 73, 65, 72, 70],\n",
    "    'ìƒëŒ€ í¬ê¸° (%)': [100, 50, 75, 75, 37.5],\n",
    "    'ì²˜ë¦¬ëŸ‰ (req/s)': [10, 18, 13, 13, 24],\n",
    "    'ë©”ëª¨ë¦¬ (GB)': [6.4, 3.2, 4.8, 4.8, 2.4]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "display(df)\n",
    "\n",
    "# CSVë¡œ ì €ì¥\n",
    "df.to_csv('results/compression_comparison.csv', index=False)\n",
    "print(\"\\nâœ… ê²°ê³¼ë¥¼ results/compression_comparison.csvì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 ì‹œê°í™”\n",
    "\n",
    "#### 6.2.1 MMLU ì •í™•ë„ ë¹„êµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "colors = ['#1f77b4', '#76b900', '#ff7f0e', '#2ca02c', '#d62728']\n",
    "bars = ax.bar(df['ëª¨ë¸'], df['MMLU (%)'], color=colors)\n",
    "\n",
    "# ê°’ í‘œì‹œ\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{height:.0f}%',\n",
    "            ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "ax.set_ylabel('MMLU ì •í™•ë„ (%)', fontsize=14, fontweight='bold')\n",
    "ax.set_title('ëª¨ë¸ ì••ì¶• ê¸°ìˆ ë³„ MMLU ì •í™•ë„ ë¹„êµ', fontsize=16, fontweight='bold')\n",
    "ax.set_ylim(0, 100)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.xticks(rotation=15, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/mmlu_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… ê·¸ë˜í”„ë¥¼ results/mmlu_comparison.pngì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.2 íŒŒë ˆí†  ê³¡ì„  (ì •í™•ë„ vs ì²˜ë¦¬ëŸ‰)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# ì‚°ì ë„\n",
    "scatter = ax.scatter(df['ì²˜ë¦¬ëŸ‰ (req/s)'], df['MMLU (%)'], \n",
    "                     s=300, c=colors, alpha=0.6, edgecolors='black', linewidth=2)\n",
    "\n",
    "# ë ˆì´ë¸” ì¶”ê°€\n",
    "for i, model in enumerate(df['ëª¨ë¸']):\n",
    "    ax.annotate(model, \n",
    "                (df['ì²˜ë¦¬ëŸ‰ (req/s)'][i], df['MMLU (%)'][i]),\n",
    "                textcoords=\"offset points\",\n",
    "                xytext=(0, 10),\n",
    "                ha='center',\n",
    "                fontsize=10,\n",
    "                fontweight='bold')\n",
    "\n",
    "# íŒŒë ˆí†  í”„ë¡ í‹°ì–´ í‘œì‹œ\n",
    "pareto_indices = [0, 1, 3, 4]  # BF16 ê¸°ì¤€, FP8, BF16 ì¦ë¥˜, FP8 ì¦ë¥˜\n",
    "pareto_df = df.iloc[pareto_indices].sort_values('ì²˜ë¦¬ëŸ‰ (req/s)')\n",
    "ax.plot(pareto_df['ì²˜ë¦¬ëŸ‰ (req/s)'], pareto_df['MMLU (%)'], \n",
    "        'r--', alpha=0.5, linewidth=2, label='íŒŒë ˆí†  í”„ë¡ í‹°ì–´')\n",
    "\n",
    "ax.set_xlabel('ì²˜ë¦¬ëŸ‰ (requests/second)', fontsize=14, fontweight='bold')\n",
    "ax.set_ylabel('MMLU ì •í™•ë„ (%)', fontsize=14, fontweight='bold')\n",
    "ax.set_title('ëª¨ë¸ ì••ì¶•: ì •í™•ë„ vs ì²˜ë¦¬ëŸ‰ (íŒŒë ˆí†  ê³¡ì„ )', fontsize=16, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/pareto_curve.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… ê·¸ë˜í”„ë¥¼ results/pareto_curve.pngì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.3 ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¹„êµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "bars = ax.barh(df['ëª¨ë¸'], df['ë©”ëª¨ë¦¬ (GB)'], color=colors)\n",
    "\n",
    "# ê°’ í‘œì‹œ\n",
    "for bar in bars:\n",
    "    width = bar.get_width()\n",
    "    ax.text(width, bar.get_y() + bar.get_height()/2.,\n",
    "            f'{width:.1f} GB',\n",
    "            ha='left', va='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "ax.set_xlabel('GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ (GB)', fontsize=14, fontweight='bold')\n",
    "ax.set_title('ëª¨ë¸ ì••ì¶• ê¸°ìˆ ë³„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¹„êµ', fontsize=16, fontweight='bold')\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/memory_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… ê·¸ë˜í”„ë¥¼ results/memory_comparison.pngì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 ì••ì¶•ë¥  ë¶„ì„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì••ì¶•ë¥  ê³„ì‚°\n",
    "df['ì••ì¶•ë¥ '] = df['ìƒëŒ€ í¬ê¸° (%)'].apply(lambda x: f\"{100 - x:.1f}%\")\n",
    "df['ì •í™•ë„ ì†ì‹¤'] = df['MMLU (%)'].apply(lambda x: f\"{75 - x:.1f}%p\")\n",
    "df['ì†ë„ í–¥ìƒ'] = df['ì²˜ë¦¬ëŸ‰ (req/s)'].apply(lambda x: f\"{x / 10:.2f}x\")\n",
    "\n",
    "summary_df = df[['ëª¨ë¸', 'ì••ì¶•ë¥ ', 'ì •í™•ë„ ì†ì‹¤', 'ì†ë„ í–¥ìƒ']]\n",
    "display(summary_df)\n",
    "\n",
    "print(\"\\nğŸ“Š ì£¼ìš” ë°œê²¬:\")\n",
    "print(\"1. FP8 ì–‘ìí™”: 50% ì••ì¶•, 2%p ì†ì‹¤, 1.8ë°° ì†ë„ í–¥ìƒ\")\n",
    "print(\"2. í”„ë£¨ë‹: 25% ì••ì¶•, 10%p ì†ì‹¤ (ì¦ë¥˜ ì „)\")\n",
    "print(\"3. ì¦ë¥˜: í”„ë£¨ë‹ ì†ì‹¤ì˜ 70% ë³µêµ¬\")\n",
    "print(\"4. FP8 + ì¦ë¥˜: 62.5% ì••ì¶•, 5%p ì†ì‹¤, 2.4ë°° ì†ë„ í–¥ìƒ\")\n",
    "print(\"\\nâœ¨ ìµœì  ì¡°í•©: FP8 ì–‘ìí™” + ì¦ë¥˜ = ìµœê³ ì˜ ì••ì¶•ë¥ ê³¼ ì„±ëŠ¥ ê· í˜•\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 ì‚¬ìš© ì‚¬ë¡€ë³„ ê¶Œì¥ì‚¬í•­"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = {\n",
    "    'ì‚¬ìš© ì‚¬ë¡€': [\n",
    "        'í´ë¼ìš°ë“œ ë°°í¬',\n",
    "        'ì—£ì§€ ì¥ì¹˜',\n",
    "        'ì‹¤ì‹œê°„ ì„œë¹„ìŠ¤',\n",
    "        'ë°°ì¹˜ ì²˜ë¦¬',\n",
    "        'ëª¨ë°”ì¼'\n",
    "    ],\n",
    "    'ê¶Œì¥ ê¸°ìˆ ': [\n",
    "        'FP8 ì–‘ìí™”',\n",
    "        'INT4 + í”„ë£¨ë‹',\n",
    "        'FP8',\n",
    "        'BF16 í”„ë£¨ë‹',\n",
    "        'INT4 + ì¦ë¥˜'\n",
    "    ],\n",
    "    'ì´ìœ ': [\n",
    "        'ë¹„ìš© ì ˆê°, ë†’ì€ ì²˜ë¦¬ëŸ‰',\n",
    "        'ë©”ëª¨ë¦¬ ì œì•½, ê²½ëŸ‰í™”',\n",
    "        'ë‚®ì€ ì§€ì—°, ì¢‹ì€ í’ˆì§ˆ',\n",
    "        'ì •í™•ë„ ìš°ì„ ',\n",
    "        'ê·¹ë„ì˜ ê²½ëŸ‰í™”'\n",
    "    ]\n",
    "}\n",
    "\n",
    "rec_df = pd.DataFrame(recommendations)\n",
    "display(rec_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ë§ˆë¬´ë¦¬\n",
    "\n",
    "### í•™ìŠµ ë‚´ìš© ì •ë¦¬\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì„ í†µí•´ ë‹¤ìŒì„ í•™ìŠµí–ˆìŠµë‹ˆë‹¤:\n",
    "\n",
    "1. **ì–‘ìí™” (Quantization)**\n",
    "   - FP8 ì–‘ìí™” ì ìš©\n",
    "   - 50% í¬ê¸° ê°ì†Œ, 2%p ì •í™•ë„ ì†ì‹¤\n",
    "   - TensorRT-LLM ì‚¬ìš©\n",
    "\n",
    "2. **í”„ë£¨ë‹ (Pruning)**\n",
    "   - ê¹Šì´ í”„ë£¨ë‹ìœ¼ë¡œ 25% ë ˆì´ì–´ ì œê±°\n",
    "   - ì´ˆê¸° 10%p ì •í™•ë„ ì†ì‹¤\n",
    "   - NeMo Framework ì‚¬ìš©\n",
    "\n",
    "3. **ì§€ì‹ ì¦ë¥˜ (Knowledge Distillation)**\n",
    "   - ì›ë³¸ ëª¨ë¸ì—ì„œ í”„ë£¨ë‹ ëª¨ë¸ë¡œ ì§€ì‹ ì „ì´\n",
    "   - 7%p ì •í™•ë„ ë³µêµ¬\n",
    "   - KL Divergence ìµœì†Œí™”\n",
    "\n",
    "4. **ì¶”ë¡  ë°°í¬**\n",
    "   - Triton Inference Server ì‚¬ìš©\n",
    "   - ì§€ì—° ì‹œê°„ ë° ì²˜ë¦¬ëŸ‰ ì¸¡ì •\n",
    "   - ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí‚¹\n",
    "\n",
    "5. **ì¢…í•© ë¶„ì„**\n",
    "   - íŒŒë ˆí†  ê³¡ì„  ë¶„ì„\n",
    "   - ì‚¬ìš© ì‚¬ë¡€ë³„ ìµœì  ê¸°ìˆ  ì„ íƒ\n",
    "   - ì••ì¶•ë¥  vs ì •í™•ë„ íŠ¸ë ˆì´ë“œì˜¤í”„\n",
    "---\n",
    "\n",
    "<center><a href=\"https://www.nvidia.com/en-us/training/\"><img src=\"https://dli-lms.s3.amazonaws.com/assets/general/DLI_Header_White.png\" width=\"400\" height=\"186\" /></a></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
