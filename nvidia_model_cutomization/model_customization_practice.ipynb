{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NVIDIA NeMo ëª¨ë¸ ì»¤ìŠ¤í„°ë§ˆì´ì œì´ì…˜ ì¢…í•© ì‹¤ìŠµ\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ NeMo Microservicesë¥¼ ì‚¬ìš©í•œ LLM í‰ê°€ ë° ë¯¸ì„¸ ì¡°ì •ì˜ ì „ì²´ ì›Œí¬í”Œë¡œìš°ë¥¼ ì‹¤ìŠµí•©ë‹ˆë‹¤.\n",
    "\n",
    "## ëª©ì°¨\n",
    "1. [í™˜ê²½ ì„¤ì •](#1.-í™˜ê²½-ì„¤ì •)\n",
    "2. [NIM ê¸°ë³¸ í‰ê°€](#2.-NIM-ê¸°ë³¸-í‰ê°€)\n",
    "3. [ë°ì´í„°ì…‹ ì¤€ë¹„](#3.-ë°ì´í„°ì…‹-ì¤€ë¹„)\n",
    "4. [Zero-Shot vs. ICL ë¹„êµ](#4.-Zero-Shot-vs.-ICL-ë¹„êµ)\n",
    "5. [LoRA ë¯¸ì„¸ ì¡°ì •](#5.-LoRA-ë¯¸ì„¸-ì¡°ì •)\n",
    "6. [ê²°ê³¼ ë¹„êµ ë° ë¶„ì„](#6.-ê²°ê³¼-ë¹„êµ-ë°-ë¶„ì„)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. í™˜ê²½ ì„¤ì •\n",
    "\n",
    "### 1.1 í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ ë° Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜\n",
    "!pip install -q requests datasets bs4 ftfy huggingface_hub matplotlib pandas\n",
    "\n",
    "print(\"âœ… íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import subprocess\n",
    "from pprint import pprint\n",
    "from typing import Dict, List\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output, display, HTML\n",
    "\n",
    "# ë°ì´í„°ì…‹ ì²˜ë¦¬\n",
    "from datasets import Dataset, load_dataset\n",
    "from bs4 import BeautifulSoup\n",
    "import ftfy\n",
    "\n",
    "# Hugging Face Hub\n",
    "from huggingface_hub import HfApi\n",
    "\n",
    "print(\"âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ import ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 NeMo Microservices ì—”ë“œí¬ì¸íŠ¸ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NeMo Microservices ì—”ë“œí¬ì¸íŠ¸\n",
    "datastore_url = \"http://nemo-datastore.local\"\n",
    "nim_url = \"http://llama3-2-3b-instruct.local\"\n",
    "eval_url = \"http://nemo-evaluator.local\"\n",
    "customizer_url = \"http://nemo-customizer.local\"\n",
    "entitystore_url = \"http://nemo-entity-store.local\"\n",
    "\n",
    "# ëª¨ë¸ ID\n",
    "NIM_model_id = \"meta/llama-3.2-3b-instruct\"\n",
    "\n",
    "# MLflow ì„¤ì •\n",
    "mlflow_uri = \"http://localhost:30090\"\n",
    "\n",
    "# NVIDIA API ì„¤ì •\n",
    "NGC_API_KEY = os.environ.get(\"NGC_API_KEY\", \"nvapi-mock-key\")\n",
    "BASE_URL = os.environ.get(\"NVIDIA_BASE_URL\", \"http://proxy/v1/chat/completions\")\n",
    "\n",
    "print(\"âœ… ì—”ë“œí¬ì¸íŠ¸ ì„¤ì • ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_service_health(name: str, url: str) -> bool:\n",
    "    \"\"\"ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            print(f\"âœ… {name}: ì •ìƒ ì‘ë™\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"âš ï¸  {name}: ìƒíƒœ ì½”ë“œ {response.status_code}\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ {name}: ì—°ê²° ì‹¤íŒ¨ - {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "print(\"ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸ ì¤‘...\\n\")\n",
    "services = [\n",
    "    (\"NeMo Evaluator\", f\"{eval_url}/health\"),\n",
    "    (\"NeMo Data Store\", f\"{datastore_url}/v1/health\"),\n",
    "    (\"NeMo Entity Store\", f\"{entitystore_url}/v1/health/ready\"),\n",
    "    (\"NeMo Customizer\", f\"{customizer_url}/health/ready\"),\n",
    "    (\"NIM (Llama 3.2 3B)\", f\"{nim_url}/v1/health/ready\"),\n",
    "]\n",
    "\n",
    "all_healthy = all(check_service_health(name, url) for name, url in services)\n",
    "\n",
    "if all_healthy:\n",
    "    print(\"\\nğŸ‰ ëª¨ë“  ì„œë¹„ìŠ¤ê°€ ì •ìƒ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤!\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸  ì¼ë¶€ ì„œë¹„ìŠ¤ê°€ ì‘ë™í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. í™˜ê²½ì„ í™•ì¸í•˜ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 MLflow ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLflow í¬íŠ¸ í¬ì›Œë”© ì„¤ì •\n",
    "try:\n",
    "    subprocess.Popen(\n",
    "        [\n",
    "            \"kubectl\",\n",
    "            \"-n\",\n",
    "            \"mlflow\",\n",
    "            \"port-forward\",\n",
    "            \"--address\",\n",
    "            \"0.0.0.0\",\n",
    "            \"service/mlflow-tracking\",\n",
    "            \"30090:80\",\n",
    "        ],\n",
    "        stdout=subprocess.DEVNULL,\n",
    "        stderr=subprocess.DEVNULL,\n",
    "        close_fds=True,\n",
    "    )\n",
    "    print(\"âœ… MLflow í¬íŠ¸ í¬ì›Œë”© ì‹œì‘ë¨\")\n",
    "    print(f\"   MLflow UI: {mlflow_uri}\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  MLflow í¬íŠ¸ í¬ì›Œë”© ì‹¤íŒ¨: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. NIM ê¸°ë³¸ í‰ê°€\n",
    "\n",
    "### 2.1 NIM ìƒíƒœ í™•ì¸ ë° ê¸°ë³¸ ì¶”ë¡ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NIM ëŒ€ê¸°\n",
    "print(\"NIM ì„œë¹„ìŠ¤ ì¤€ë¹„ ëŒ€ê¸° ì¤‘...\")\n",
    "while True:\n",
    "    try:\n",
    "        response = requests.get(f\"{nim_url}/v1/health/ready\")\n",
    "        if response.status_code == 200:\n",
    "            clear_output(wait=True)\n",
    "            print(\"âœ… NIM ì„œë¹„ìŠ¤ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!\")\n",
    "            break\n",
    "        else:\n",
    "            clear_output(wait=True)\n",
    "            print(f\"ëŒ€ê¸° ì¤‘... (ìƒíƒœ ì½”ë“œ: {response.status_code})\")\n",
    "    except:\n",
    "        clear_output(wait=True)\n",
    "        print(\"ëŒ€ê¸° ì¤‘... (ì—°ê²° ì‹œë„)\")\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°°í¬ëœ ëª¨ë¸ í™•ì¸\n",
    "response = requests.get(f\"{nim_url}/v1/models\")\n",
    "models = response.json()[\"data\"]\n",
    "\n",
    "print(\"ë°°í¬ëœ ëª¨ë¸:\")\n",
    "for model in models:\n",
    "    print(f\"  - {model['id']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê¸°ë³¸ ì¶”ë¡  í…ŒìŠ¤íŠ¸\n",
    "def query_nim(prompt: str, model: str = NIM_model_id, max_tokens: int = 100) -> str:\n",
    "    \"\"\"NIMì— ì¿¼ë¦¬í•˜ì—¬ ì‘ë‹µ ë°›ê¸°\"\"\"\n",
    "    request_body = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"Be succinct in your response.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": 0.2,\n",
    "    }\n",
    "    \n",
    "    response = requests.post(f\"{nim_url}/v1/chat/completions\", json=request_body)\n",
    "    return response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬\n",
    "test_prompt = \"What is the capital of Spain?\"\n",
    "response = query_nim(test_prompt)\n",
    "\n",
    "print(f\"ì§ˆë¬¸: {test_prompt}\")\n",
    "print(f\"ë‹µë³€: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 GSM8K ë²¤ì¹˜ë§ˆí¬ í‰ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# GSM8K ë²¤ì¹˜ë§ˆí¬ í‰ê°€ (10ê°œ ìƒ˜í”Œ)\n",
    "lm_eval --model local-completions \\\n",
    "        --tasks gsm8k \\\n",
    "        --output_path ./eval_results/gsm8k \\\n",
    "        --model_args model=meta/llama-3.2-3b-instruct,base_url=http://llama3-2-3b-instruct.local/v1/completions \\\n",
    "        --limit 10 | awk -F'|' '/acc/ {gsub(/^ +| +$/, \"\", $8); print \"GSM8K Accuracy:\", $8}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 LLM-as-a-Judge í‰ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faithfulness í”„ë¡¬í”„íŠ¸ ì •ì˜\n",
    "prompt_faithfulness = \"\"\"\n",
    "System: You are an impartial judge assessing the faithfulness of an AI-generated response.\n",
    "\n",
    "Evaluation Criteria:\n",
    "1. Correctness: Does the response correctly reflect facts from the ground truth?\n",
    "2. No Hallucination: Does it introduce false information?\n",
    "3. Completeness: Does it omit key facts?\n",
    "4. Paraphrase Accuracy: Is the meaning preserved?\n",
    "\n",
    "Provide a faithfulness score from 1 (poor) to 5 (perfect) with brief explanation.\n",
    "\"\"\"\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸ ì˜ˆì œ\n",
    "question = \"What are the three most populated countries worldwide?\"\n",
    "ground_truth = \"India, China and the United States\"\n",
    "generated_answer = \"India, China and Spain\"  # ì˜ë„ì  ì˜¤ë¥˜\n",
    "\n",
    "# LLM-as-a-Judgeë¡œ í‰ê°€\n",
    "judge_response = query_nim(\n",
    "    prompt=f\"{prompt_faithfulness}\\n\\nQuestion: {question}\\nGround truth: {ground_truth}\\nAnswer: {generated_answer}\",\n",
    "    max_tokens=200\n",
    ")\n",
    "\n",
    "print(\"LLM-as-a-Judge í‰ê°€ ê²°ê³¼:\")\n",
    "print(judge_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ë°ì´í„°ì…‹ ì¤€ë¹„\n",
    "\n",
    "### 3.1 ë²•ë¥  ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ë° ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Law StackExchange ë°ì´í„°ì…‹ ë¡œë“œ\n",
    "print(\"ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì¤‘...\")\n",
    "hf_ds = load_dataset(\"ymoslem/Law-StackExchange\")\n",
    "hf_ds = hf_ds[\"train\"].remove_columns(\n",
    "    [\"link\", \"license\", \"question_id\", \"answers\", \"tags\"]\n",
    ")\n",
    "\n",
    "print(f\"ì „ì²´ ë°ì´í„°ì…‹ í¬ê¸°: {len(hf_ds)} ìƒ˜í”Œ\")\n",
    "print(\"\\nìƒ˜í”Œ ì˜ˆì‹œ:\")\n",
    "pprint(hf_ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° í•„í„°ë§ (ê³ í’ˆì§ˆ ìƒ˜í”Œ ì„ ë³„)\n",
    "print(\"ë°ì´í„° í•„í„°ë§ ì¤‘...\")\n",
    "filtered_ds = hf_ds.filter(lambda row: row[\"score\"] > 10)\n",
    "filtered_ds = filtered_ds.filter(\n",
    "    lambda row: len(row[\"question_body\"]) >= 50 and len(row[\"question_body\"]) <= 1000\n",
    ")\n",
    "\n",
    "print(f\"í•„í„°ë§ í›„ í¬ê¸°: {len(filtered_ds)} ìƒ˜í”Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML íƒœê·¸ ì œê±° ë° í…ìŠ¤íŠ¸ ì •ë¦¬\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"HTML íƒœê·¸ ì œê±° ë° í…ìŠ¤íŠ¸ ì •ê·œí™”\"\"\"\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    text = ftfy.fix_text(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "print(\"í…ìŠ¤íŠ¸ ì •ë¦¬ ì¤‘...\")\n",
    "modified_ds = filtered_ds.map(\n",
    "    lambda row: {\n",
    "        \"question_body\": clean_text(row[\"question_body\"]),\n",
    "        \"question_title\": clean_text(row[\"question_title\"]),\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"\\nì •ë¦¬ í›„ ìƒ˜í”Œ:\")\n",
    "pprint(modified_ds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NeMo í˜•ì‹ìœ¼ë¡œ ë³€í™˜\n",
    "nemo_ds = (\n",
    "    modified_ds.rename_column(\"question_body\", \"prompt\")\n",
    "    .rename_column(\"question_title\", \"completion\")\n",
    "    .remove_columns([\"score\"])\n",
    ")\n",
    "\n",
    "print(\"NeMo í˜•ì‹ ë³€í™˜ ì™„ë£Œ\")\n",
    "print(f\"ìµœì¢… ë°ì´í„°ì…‹: {nemo_ds}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 ë°ì´í„°ì…‹ ë¶„í• "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í›ˆë ¨/ê²€ì¦/í…ŒìŠ¤íŠ¸ ë¶„í• \n",
    "ds = nemo_ds.train_test_split(test_size=0.5, shuffle=False)\n",
    "training_dataset = ds[\"train\"]\n",
    "\n",
    "ds = ds[\"test\"].train_test_split(test_size=0.1, shuffle=False)\n",
    "validation_dataset = ds[\"train\"]\n",
    "evaluation_dataset = ds[\"test\"]\n",
    "\n",
    "print(\"ë°ì´í„°ì…‹ ë¶„í•  ì™„ë£Œ:\")\n",
    "print(f\"  í›ˆë ¨: {len(training_dataset)} ìƒ˜í”Œ\")\n",
    "print(f\"  ê²€ì¦: {len(validation_dataset)} ìƒ˜í”Œ\")\n",
    "print(f\"  í…ŒìŠ¤íŠ¸: {len(evaluation_dataset)} ìƒ˜í”Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Data Storeì— ì—…ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„°ì…‹ ì €ì¥\n",
    "dataset_name = \"legal_dataset\"\n",
    "os.makedirs(\"./datasets\", exist_ok=True)\n",
    "\n",
    "training_dataset.to_json(\"./datasets/training.jsonl\")\n",
    "validation_dataset.to_json(\"./datasets/validation.jsonl\")\n",
    "evaluation_dataset.to_json(\"./datasets/testing.jsonl\")\n",
    "\n",
    "print(\"âœ… ë¡œì»¬ì— ë°ì´í„°ì…‹ ì €ì¥ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HuggingFace Hubì„ í†µí•´ NeMo Data Storeì— ì—…ë¡œë“œ\n",
    "try:\n",
    "    api = HfApi(endpoint=datastore_url)\n",
    "    repo_id = f\"default/{dataset_name}\"\n",
    "    \n",
    "    # í›ˆë ¨ ë°ì´í„° ì—…ë¡œë“œ\n",
    "    api.upload_file(\n",
    "        path_or_fileobj=\"./datasets/training.jsonl\",\n",
    "        path_in_repo=\"training/training.jsonl\",\n",
    "        repo_id=repo_id,\n",
    "        repo_type=\"dataset\"\n",
    "    )\n",
    "    \n",
    "    # ê²€ì¦ ë°ì´í„° ì—…ë¡œë“œ\n",
    "    api.upload_file(\n",
    "        path_or_fileobj=\"./datasets/validation.jsonl\",\n",
    "        path_in_repo=\"validation/validation.jsonl\",\n",
    "        repo_id=repo_id,\n",
    "        repo_type=\"dataset\"\n",
    "    )\n",
    "    \n",
    "    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì—…ë¡œë“œ\n",
    "    api.upload_file(\n",
    "        path_or_fileobj=\"./datasets/testing.jsonl\",\n",
    "        path_in_repo=\"testing/testing.jsonl\",\n",
    "        repo_id=repo_id,\n",
    "        repo_type=\"dataset\"\n",
    "    )\n",
    "    \n",
    "    print(f\"âœ… NeMo Data Storeì— ì—…ë¡œë“œ ì™„ë£Œ: {repo_id}\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  ì—…ë¡œë“œ ì‹¤íŒ¨: {e}\")\n",
    "    print(\"   ë¡œì»¬ íŒŒì¼ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Zero-Shot vs. ICL ë¹„êµ\n",
    "\n",
    "### 4.1 Few-Shot ì˜ˆì œ ì¤€ë¹„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few-Shot ì˜ˆì œ ì„ íƒ (ICLìš©)\n",
    "few_shot_examples = [\n",
    "    {\n",
    "        \"question\": \"Do police officers have to stop an interrogation when right to counsel has been invoked?\",\n",
    "        \"title\": \"Must the interrogation stop when the right to counsel has been invoked by a suspect?\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What constitutes workplace discrimination under employment law?\",\n",
    "        \"title\": \"Workplace Discrimination: Legal Definitions and Protected Characteristics\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"How does copyright law apply to software and source code?\",\n",
    "        \"title\": \"Copyright Protection for Software: Source Code and Object Code\"\n",
    "    },\n",
    "]\n",
    "\n",
    "# Few-Shot í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
    "def create_few_shot_prompt(question: str, examples: List[Dict]) -> str:\n",
    "    \"\"\"Few-Shot í”„ë¡¬í”„íŠ¸ ìƒì„±\"\"\"\n",
    "    prompt = \"Generate a concise, engaging title for the following legal questions.\\n\\n\"\n",
    "    \n",
    "    # ì˜ˆì œ ì¶”ê°€\n",
    "    for i, ex in enumerate(examples, 1):\n",
    "        prompt += f\"Example {i}:\\n\"\n",
    "        prompt += f\"QUESTION: {ex['question']}\\n\"\n",
    "        prompt += f\"TITLE: {ex['title']}\\n\\n\"\n",
    "    \n",
    "    # ì‹¤ì œ ì§ˆë¬¸\n",
    "    prompt += f\"QUESTION: {question}\\nTITLE:\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "print(\"âœ… Few-Shot í”„ë¡¬í”„íŠ¸ ìƒì„± í•¨ìˆ˜ ì¤€ë¹„ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Zero-Shot vs. Few-Shot ë¹„êµ í‰ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í…ŒìŠ¤íŠ¸ ìƒ˜í”Œ ì„ íƒ\n",
    "test_sample = evaluation_dataset[0]\n",
    "test_question = test_sample[\"prompt\"]\n",
    "ground_truth = test_sample[\"completion\"]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"í…ŒìŠ¤íŠ¸ ì§ˆë¬¸:\")\n",
    "print(test_question[:200] + \"...\")\n",
    "print(\"\\nì •ë‹µ (Ground Truth):\")\n",
    "print(ground_truth)\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-Shot ì¶”ë¡ \n",
    "zero_shot_prompt = f\"Generate a concise, engaging title for the following legal question.\\n\\nQUESTION: {test_question}\\nTITLE:\"\n",
    "\n",
    "zero_shot_response = query_nim(zero_shot_prompt, max_tokens=50)\n",
    "\n",
    "print(\"\\nğŸ”¹ Zero-Shot ê²°ê³¼:\")\n",
    "print(zero_shot_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Few-Shot (ICL) ì¶”ë¡ \n",
    "few_shot_prompt = create_few_shot_prompt(test_question, few_shot_examples)\n",
    "\n",
    "few_shot_response = query_nim(few_shot_prompt, max_tokens=50)\n",
    "\n",
    "print(\"\\nğŸ”¹ Few-Shot (ICL) ê²°ê³¼:\")\n",
    "print(few_shot_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²°ê³¼ ë¹„êµ\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ê²°ê³¼ ìš”ì•½\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Ground Truth:  {ground_truth}\")\n",
    "print(f\"Zero-Shot:     {zero_shot_response}\")\n",
    "print(f\"Few-Shot (ICL): {few_shot_response}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. LoRA ë¯¸ì„¸ ì¡°ì •\n",
    "\n",
    "### 5.1 ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì„¤ì • í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customizerì—ì„œ ì§€ì›í•˜ëŠ” ëª¨ë¸ í™•ì¸\n",
    "response = requests.get(\n",
    "    f\"{customizer_url}/v1/customization/configs\",\n",
    "    headers={\"accept\": \"application/json\"},\n",
    "    timeout=30\n",
    ")\n",
    "\n",
    "print(\"ì§€ì›ë˜ëŠ” ëª¨ë¸ ì„¤ì •:\")\n",
    "print(json.dumps(response.json(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 LoRA ë¯¸ì„¸ ì¡°ì • ì‘ì—… ì‹œì‘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA ë¯¸ì„¸ ì¡°ì • ì„¤ì •\n",
    "training_config = {\n",
    "    \"config\": \"meta/llama-3.2-3b-instruct\",\n",
    "    \"dataset\": {\"name\": dataset_name},\n",
    "    \"hyperparameters\": {\n",
    "        \"training_type\": \"sft\",  # Supervised Fine-Tuning\n",
    "        \"finetuning_type\": \"lora\",\n",
    "        \"epochs\": 3,\n",
    "        \"batch_size\": 32,\n",
    "        \"learning_rate\": 1e-4,\n",
    "        \"lora\": {\n",
    "            \"adapter_dim\": 8,      # LoRA rank\n",
    "            \"adapter_dropout\": 0.1,\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"ë¯¸ì„¸ ì¡°ì • ì„¤ì •:\")\n",
    "pprint(training_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¯¸ì„¸ ì¡°ì • ì‘ì—… ì‹œì‘\n",
    "response = requests.post(\n",
    "    f\"{customizer_url}/v1/customization/jobs\",\n",
    "    headers={\"Content-Type\": \"application/json\"},\n",
    "    json=training_config,\n",
    "    timeout=30\n",
    ")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    job_data = response.json()\n",
    "    job_id = job_data[\"id\"]\n",
    "    print(f\"âœ… ë¯¸ì„¸ ì¡°ì • ì‘ì—… ì‹œì‘ë¨: {job_id}\")\n",
    "else:\n",
    "    print(f\"âŒ ì‘ì—… ì‹œì‘ ì‹¤íŒ¨: {response.status_code}\")\n",
    "    print(response.text)\n",
    "    job_id = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 í•™ìŠµ ì§„í–‰ ëª¨ë‹ˆí„°ë§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•™ìŠµ ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§\n",
    "if job_id:\n",
    "    import sys\n",
    "    \n",
    "    status = \"initializing\"\n",
    "    refresh_interval = 30\n",
    "    \n",
    "    print(\"í•™ìŠµ ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§ ì¤‘...\\n\")\n",
    "    \n",
    "    while status in {\"initializing\", \"running\", \"created\"}:\n",
    "        job = requests.get(\n",
    "            f\"{customizer_url}/v1/customization/jobs/{job_id}\",\n",
    "            timeout=10\n",
    "        ).json()\n",
    "        \n",
    "        status = job[\"status\"]\n",
    "        \n",
    "        # ì¹´ìš´íŠ¸ë‹¤ìš´ íƒ€ì´ë¨¸ë¡œ ìƒíƒœ í‘œì‹œ\n",
    "        for remaining in range(refresh_interval, 0, -1):\n",
    "            sys.stdout.write('\\033[K')  # ë¼ì¸ ì§€ìš°ê¸°\n",
    "            sys.stdout.write(f\"\\râ³  ìƒíƒœ: {status:<12} (ë‹¤ìŒ í™•ì¸: {remaining}ì´ˆ)\")\n",
    "            sys.stdout.flush()\n",
    "            time.sleep(1)\n",
    "    \n",
    "    # ìµœì¢… ìƒíƒœ\n",
    "    sys.stdout.write('\\033[K')\n",
    "    print(f\"\\n\\nğŸ  ì‘ì—… ì™„ë£Œ â†’ {job['status']}\")\n",
    "    \n",
    "    if job['status'] == 'completed':\n",
    "        output_model = job[\"output_model\"]\n",
    "        print(f\"\\nâœ… ë¯¸ì„¸ ì¡°ì •ëœ ëª¨ë¸: {output_model}\")\n",
    "    else:\n",
    "        print(f\"\\nâŒ ì‘ì—… ì‹¤íŒ¨: {job.get('status_details', {}).get('error_message', 'Unknown error')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 í•™ìŠµ ë©”íŠ¸ë¦­ ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•™ìŠµ ë©”íŠ¸ë¦­ ì‹œê°í™”\n",
    "if job_id and job.get('status') == 'completed':\n",
    "    metrics = job['status_details']['metrics']['metrics']\n",
    "    \n",
    "    # Training & Validation Loss\n",
    "    train_loss = pd.DataFrame(metrics['train_loss'])\n",
    "    val_loss = pd.DataFrame(metrics['val_loss'])\n",
    "    \n",
    "    # ìš”ì•½ í†µê³„\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"í•™ìŠµ ìš”ì•½\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"ìµœì¢… Training Loss:  {train_loss['value'].iloc[-1]:.4f}\")\n",
    "    print(f\"ìµœì  Training Loss:  {train_loss['value'].min():.4f}\")\n",
    "    print(f\"ìµœì  Validation Loss: {val_loss['value'].min():.4f}\")\n",
    "    print(f\"ì´ í•™ìŠµ ìŠ¤í…:        {train_loss['step'].max()}\")\n",
    "    print(f\"ì™„ë£Œëœ Epoch:        {job['status_details']['epochs_completed']}\")\n",
    "    print(f\"ìµœì  Epoch:          {job['status_details']['best_epoch']}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # ê·¸ë˜í”„ ì‹œê°í™”\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(train_loss['step'], train_loss['value'], label='Training Loss', linewidth=2)\n",
    "    plt.scatter(val_loss['step'], val_loss['value'], color='red', label='Validation Loss', s=50)\n",
    "    plt.xlabel('Training Steps', fontsize=12)\n",
    "    plt.ylabel('Loss', fontsize=12)\n",
    "    plt.title('LoRA ë¯¸ì„¸ ì¡°ì • ì§„í–‰ ìƒí™©', fontsize=14, fontweight='bold')\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ê²°ê³¼ ë¹„êµ ë° ë¶„ì„\n",
    "\n",
    "### 6.1 ë¯¸ì„¸ ì¡°ì • ëª¨ë¸ í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë¯¸ì„¸ ì¡°ì • ëª¨ë¸ë¡œ ì¶”ë¡ \n",
    "if 'output_model' in locals():\n",
    "    lora_prompt = f\"Generate a concise, engaging title for the following legal question.\\n\\nQUESTION: {test_question}\\nTITLE:\"\n",
    "    \n",
    "    lora_response = query_nim(lora_prompt, model=output_model, max_tokens=50)\n",
    "    \n",
    "    print(\"\\nğŸ”¹ LoRA ë¯¸ì„¸ ì¡°ì • ëª¨ë¸ ê²°ê³¼:\")\n",
    "    print(lora_response)\n",
    "else:\n",
    "    print(\"ë¯¸ì„¸ ì¡°ì • ëª¨ë¸ì´ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "    lora_response = \"(ë¯¸ì„¸ ì¡°ì • ë¯¸ì™„ë£Œ)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 ì¢…í•© ë¹„êµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²°ê³¼ ì¢…í•© ë¹„êµ\n",
    "comparison_df = pd.DataFrame({\n",
    "    \"ì ‘ê·¼ ë°©ì‹\": [\"Ground Truth\", \"Zero-Shot\", \"Few-Shot (ICL)\", \"LoRA Fine-tuned\"],\n",
    "    \"ìƒì„±ëœ ì œëª©\": [\n",
    "        ground_truth,\n",
    "        zero_shot_response,\n",
    "        few_shot_response,\n",
    "        lora_response\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"ì¢…í•© ë¹„êµ ê²°ê³¼\")\n",
    "print(\"=\"*100)\n",
    "display(comparison_df)\n",
    "print(\"=\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë¹„êµ (ê°€ìƒ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì„±ëŠ¥ ë©”íŠ¸ë¦­ ë¹„êµ ì°¨íŠ¸ (ì˜ˆì‹œ)\n",
    "performance_data = {\n",
    "    \"ì ‘ê·¼ ë°©ì‹\": [\"Zero-Shot\", \"Few-Shot (ICL)\", \"LoRA Fine-tuned\"],\n",
    "    \"ROUGE Score\": [0.35, 0.52, 0.68],\n",
    "    \"BLEU Score\": [0.28, 0.45, 0.61],\n",
    "    \"F1 Score\": [0.31, 0.48, 0.64],\n",
    "}\n",
    "\n",
    "perf_df = pd.DataFrame(performance_data)\n",
    "\n",
    "# ë§‰ëŒ€ ê·¸ë˜í”„\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "x = range(len(perf_df[\"ì ‘ê·¼ ë°©ì‹\"]))\n",
    "width = 0.25\n",
    "\n",
    "ax.bar([i - width for i in x], perf_df[\"ROUGE Score\"], width, label='ROUGE Score')\n",
    "ax.bar(x, perf_df[\"BLEU Score\"], width, label='BLEU Score')\n",
    "ax.bar([i + width for i in x], perf_df[\"F1 Score\"], width, label='F1 Score')\n",
    "\n",
    "ax.set_xlabel('ì ‘ê·¼ ë°©ì‹', fontsize=12)\n",
    "ax.set_ylabel('ì ìˆ˜', fontsize=12)\n",
    "ax.set_title('í‰ê°€ ë©”íŠ¸ë¦­ ë¹„êµ (ì˜ˆì‹œ)', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(perf_df[\"ì ‘ê·¼ ë°©ì‹\"])\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nì„±ëŠ¥ ë©”íŠ¸ë¦­ í‘œ:\")\n",
    "display(perf_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 ê²°ë¡  ë° ê¶Œì¥ ì‚¬í•­"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "==================================================================================\n",
    "ê²°ë¡  ë° ê¶Œì¥ ì‚¬í•­\n",
    "==================================================================================\n",
    "\n",
    "1. Zero-Shot\n",
    "   ì¥ì : ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥, ì¶”ê°€ ë¦¬ì†ŒìŠ¤ ë¶ˆí•„ìš”\n",
    "   ë‹¨ì : ë„ë©”ì¸ íŠ¹í™” ì‘ì—…ì—ì„œ ì„±ëŠ¥ ì œí•œì \n",
    "   ì ìš©: ì¼ë°˜ì ì¸ ì§ˆë¬¸, í”„ë¡œí† íƒ€ì´í•‘\n",
    "\n",
    "2. Few-Shot (ICL)\n",
    "   ì¥ì : ë¹ ë¥¸ ì ì‘, ëª¨ë¸ ìˆ˜ì • ë¶ˆí•„ìš”\n",
    "   ë‹¨ì : ì»¨í…ìŠ¤íŠ¸ ì°½ í¬ê¸° ì œí•œ, ì„ì‹œì  ê°œì„ \n",
    "   ì ìš©: ì¤‘ê°„ ë³µì¡ë„ ì‘ì—…, ë¹ ë¥¸ ì‹¤í—˜\n",
    "\n",
    "3. LoRA ë¯¸ì„¸ ì¡°ì •\n",
    "   ì¥ì : ìµœê³  ì„±ëŠ¥, ì˜êµ¬ì  ê°œì„ , íš¨ìœ¨ì  í•™ìŠµ\n",
    "   ë‹¨ì : í•™ìŠµ ì‹œê°„ í•„ìš”, ê³ í’ˆì§ˆ ë°ì´í„° í•„ìš”\n",
    "   ì ìš©: í”„ë¡œë•ì…˜ ë°°í¬, ë„ë©”ì¸ íŠ¹í™” ì• í”Œë¦¬ì¼€ì´ì…˜\n",
    "\n",
    "ê¶Œì¥ ì›Œí¬í”Œë¡œìš°:\n",
    "1. Zero-Shotìœ¼ë¡œ ë² ì´ìŠ¤ë¼ì¸ ì„±ëŠ¥ í™•ì¸\n",
    "2. Few-Shotìœ¼ë¡œ ë¹ ë¥¸ ì„±ëŠ¥ ê°œì„  í…ŒìŠ¤íŠ¸\n",
    "3. í”„ë¡œë•ì…˜ ë°°í¬ ì „ LoRA ë¯¸ì„¸ ì¡°ì • ìˆ˜í–‰\n",
    "4. MLflowë¡œ ëª¨ë“  ì‹¤í—˜ ì¶”ì  ë° ë¹„êµ\n",
    "==================================================================================\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ë§ˆë¬´ë¦¬\n",
    "\n",
    "### ë¦¬ì†ŒìŠ¤ ì •ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‚¬ìš©í•œ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ (ì„ íƒ ì‚¬í•­)\n",
    "cleanup = input(\"ë¦¬ì†ŒìŠ¤ë¥¼ ì •ë¦¬í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/n): \")\n",
    "\n",
    "if cleanup.lower() == 'y':\n",
    "    print(\"\\në¦¬ì†ŒìŠ¤ ì •ë¦¬ ì¤‘...\")\n",
    "    \n",
    "    # ì„ì‹œ íŒŒì¼ ì‚­ì œ\n",
    "    import shutil\n",
    "    if os.path.exists(\"./datasets\"):\n",
    "        shutil.rmtree(\"./datasets\")\n",
    "    if os.path.exists(\"./eval_results\"):\n",
    "        shutil.rmtree(\"./eval_results\")\n",
    "    \n",
    "    print(\"âœ… ë¡œì»¬ íŒŒì¼ ì •ë¦¬ ì™„ë£Œ\")\n",
    "    print(\"\\nKubernetes ë¦¬ì†ŒìŠ¤ë¥¼ ì •ë¦¬í•˜ë ¤ë©´ ë‹¤ìŒ ëª…ë ¹ì„ ì‹¤í–‰í•˜ì„¸ìš”:\")\n",
    "    print(\"  kubectl delete namespace llama3-2-3b-instruct\")\n",
    "else:\n",
    "    print(\"\\në¦¬ì†ŒìŠ¤ê°€ ìœ ì§€ë©ë‹ˆë‹¤.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
