{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NVIDIA NeMo Curator ë°ì´í„° íë ˆì´ì…˜ ì¢…í•© ì‹¤ìŠµ\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì€ NeMo Curatorë¥¼ ì‚¬ìš©í•œ ë°ì´í„° íë ˆì´ì…˜ê³¼ í•©ì„± ë°ì´í„° ìƒì„±ì˜ ì „ì²´ ì›Œí¬í”Œë¡œìš°ë¥¼ ì‹¤ìŠµí•©ë‹ˆë‹¤.\n",
    "\n",
    "## ëª©ì°¨\n",
    "1. [í™˜ê²½ ì„¤ì •](#1.-í™˜ê²½-ì„¤ì •)\n",
    "2. [ê¸°ë³¸ ë°ì´í„° íë ˆì´ì…˜](#2.-ê¸°ë³¸-ë°ì´í„°-íë ˆì´ì…˜)\n",
    "3. [ì£¼ì œ ë° ë¶€ì£¼ì œ ìƒì„±](#3.-ì£¼ì œ-ë°-ë¶€ì£¼ì œ-ìƒì„±)\n",
    "4. [Q&A ë°ì´í„°ì…‹ ìƒì„±](#4.-Q&A-ë°ì´í„°ì…‹-ìƒì„±)\n",
    "5. [ìˆ˜í•™ ë¬¸ì œ ìƒì„±](#5.-ìˆ˜í•™-ë¬¸ì œ-ìƒì„±)\n",
    "6. [ì¢…í•© ì‹¤ìŠµ](#6.-ì¢…í•©-ì‹¤ìŠµ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. í™˜ê²½ ì„¤ì •\n",
    "\n",
    "### 1.1 í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import re\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "# NeMo Curator\n",
    "from nemo_curator import OpenAIClient, Sequential, ScoreFilter\n",
    "from nemo_curator.datasets import DocumentDataset\n",
    "from nemo_curator.modifiers import DocumentModifier, UnicodeReformatter\n",
    "from nemo_curator.modifiers.pii_modifier import PiiModifier\n",
    "from nemo_curator.modules.modify import Modify\n",
    "from nemo_curator.filters import WordCountFilter, DocumentFilter\n",
    "from nemo_curator.synthetic import NemotronGenerator\n",
    "from nemo_curator.synthetic.error import YamlConversionError\n",
    "from nemo_curator.utils.distributed_utils import get_client\n",
    "\n",
    "# OpenAI\n",
    "from openai import OpenAI\n",
    "\n",
    "# Others\n",
    "import pandas as pd\n",
    "import dask\n",
    "import dask.dataframe\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print(\"âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ import ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Dask í´ëŸ¬ìŠ¤í„° ì´ˆê¸°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU ê¸°ë°˜ Dask í´ëŸ¬ìŠ¤í„° ì‹œì‘\n",
    "client = get_client(cluster_type=\"cpu\")\n",
    "print(f\"âœ… Dask í´ëŸ¬ìŠ¤í„° ì‹œì‘ë¨: {client}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 NVIDIA API í´ë¼ì´ì–¸íŠ¸ ì„¤ì •\n",
    "\n",
    "> **ì°¸ê³ **: API í‚¤ëŠ” í™˜ê²½ë³€ìˆ˜ë¡œ ì„¤ì •ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”\n",
    "openai_client = OpenAI(\n",
    "    base_url=os.environ.get(\"NVIDIA_BASE_URL\", \"https://integrate.api.nvidia.com/v1\"),\n",
    "    api_key=os.environ.get(\"NVIDIA_API_KEY\", \"your-api-key-here\"),\n",
    ")\n",
    "\n",
    "# NeMo Curator í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”\n",
    "curator_client = OpenAIClient(openai_client)\n",
    "generator = NemotronGenerator(curator_client)\n",
    "\n",
    "# ëª¨ë¸ ì„¤ì •\n",
    "model = \"mistralai/mistral-7b-instruct-v0.3\"\n",
    "model_kwargs = {\n",
    "    \"temperature\": 0.1,\n",
    "    \"top_p\": 0.9,\n",
    "    \"max_tokens\": 1024,\n",
    "}\n",
    "\n",
    "print(\"âœ… API í´ë¼ì´ì–¸íŠ¸ ì„¤ì • ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API ì—°ê²° í…ŒìŠ¤íŠ¸\n",
    "test_response = curator_client.query_model(\n",
    "    model=\"nvidia/nemotron-mini-4b-instruct\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Hello! Please respond in Korean.\"}],\n",
    "    temperature=0.2,\n",
    "    top_p=0.7,\n",
    "    max_tokens=100,\n",
    ")\n",
    "\n",
    "print(\"API í…ŒìŠ¤íŠ¸ ì‘ë‹µ:\")\n",
    "print(test_response[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ê¸°ë³¸ ë°ì´í„° íë ˆì´ì…˜\n",
    "\n",
    "### 2.1 ì»¤ìŠ¤í…€ í…ìŠ¤íŠ¸ í´ë¦¬ë„ˆ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuotationTagUnifier(DocumentModifier):\n",
    "    \"\"\"ë”°ì˜´í‘œ í†µì¼ ë° HTML/URL íƒœê·¸ ì œê±°\"\"\"\n",
    "    def modify_document(self, text: str) -> str:\n",
    "        # ë”°ì˜´í‘œ ì •ê·œí™”\n",
    "        text = text.replace(\"'\", \"'\").replace(\"'\", \"'\")\n",
    "        text = text.replace(\""\", '\"').replace(\""\", '\"')\n",
    "        # íƒ­ ì œê±°\n",
    "        text = text.replace(\"\\t\", \" \")\n",
    "        # HTML íƒœê·¸ ë° URL ì œê±°\n",
    "        text = re.sub(\n",
    "            r\"(<[^>]+>)|(http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+)\",\n",
    "            \"\",\n",
    "            text,\n",
    "        )\n",
    "        return text\n",
    "\n",
    "\n",
    "class IncompleteDocumentFilter(DocumentFilter):\n",
    "    \"\"\"ë¶ˆì™„ì „í•œ ë¬¸ì„œ í•„í„°ë§\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self._story_terminators = {\".\", \"!\", \"?\", '\"', \"\"\"}\n",
    "\n",
    "    def score_document(self, text: str) -> bool:\n",
    "        return text.strip()[-1] in self._story_terminators\n",
    "\n",
    "    def keep_document(self, score) -> bool:\n",
    "        return score\n",
    "\n",
    "\n",
    "print(\"âœ… ì»¤ìŠ¤í…€ í´ë¦¬ë„ˆ ë° í•„í„° ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 ìƒ˜í”Œ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìƒ˜í”Œ ë°ì´í„° ìƒì„±\n",
    "sample_data = pd.DataFrame({\n",
    "    \"text\": [\n",
    "        \"Ryanì€ 'football'ì„ í•˜ëŸ¬ ë‚˜ê°”ìŠµë‹ˆë‹¤. ê·¸ëŠ” ë§¤ìš° í–‰ë³µí–ˆìŠµë‹ˆë‹¤.\",\n",
    "        \"Visit <a href='www.example.com'>example.com</a> for more info or email info@example.com\",\n",
    "        \"ì´ê²ƒì€ ì™„ì „í•œ ë¬¸ì¥ì…ë‹ˆë‹¤.\",\n",
    "        \"ì´ê²ƒì€ ë¶ˆì™„ì „\",  # í•„í„°ë§ë  ì˜ˆì •\n",
    "    ]\n",
    "})\n",
    "\n",
    "sample_dataset = DocumentDataset(dask.dataframe.from_pandas(sample_data, npartitions=1))\n",
    "\n",
    "print(\"ì›ë³¸ ë°ì´í„°:\")\n",
    "display(sample_dataset.df.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì •ë¦¬ íŒŒì´í”„ë¼ì¸ ì ìš©\n",
    "cleaners = Sequential([\n",
    "    Modify(QuotationTagUnifier()),\n",
    "    Modify(UnicodeReformatter()),\n",
    "])\n",
    "\n",
    "cleaned_dataset = cleaners(sample_dataset).persist()\n",
    "\n",
    "print(\"\\nì •ë¦¬ëœ ë°ì´í„°:\")\n",
    "display(cleaned_dataset.df.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„í„°ë§ ì ìš©\n",
    "filters = Sequential([\n",
    "    ScoreFilter(IncompleteDocumentFilter(), text_field=\"text\"),\n",
    "])\n",
    "\n",
    "filtered_dataset = filters(cleaned_dataset).persist()\n",
    "\n",
    "print(\"\\ní•„í„°ë§ëœ ë°ì´í„°:\")\n",
    "display(filtered_dataset.df.compute())\n",
    "print(f\"\\ní•„í„°ë§ í›„ ë¬¸ì„œ ìˆ˜: {len(filtered_dataset.df)}ê°œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 PII ì œê±° í…ŒìŠ¤íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PIIê°€ í¬í•¨ëœ ìƒ˜í”Œ ë°ì´í„°\n",
    "pii_data = pd.DataFrame({\n",
    "    \"text\": [\n",
    "        \"ê¹€ì² ìˆ˜ì˜ ì´ë©”ì¼ì€ kim@example.comì´ê³  ì „í™”ë²ˆí˜¸ëŠ” 010-1234-5678ì…ë‹ˆë‹¤.\",\n",
    "        \"My name is John Doe and my number is 212-555-5555\",\n",
    "    ]\n",
    "})\n",
    "\n",
    "pii_dataset = DocumentDataset(dask.dataframe.from_pandas(pii_data, npartitions=1))\n",
    "\n",
    "print(\"PII ì œê±° ì „:\")\n",
    "display(pii_dataset.df.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PII ì œê±° (ì˜ì–´ë§Œ ì§€ì›)\n",
    "pii_modifier = Modify(\n",
    "    PiiModifier(\n",
    "        supported_entities=[\"PERSON\", \"EMAIL_ADDRESS\", \"PHONE_NUMBER\"],\n",
    "        anonymize_action=\"replace\",\n",
    "        language=\"en\",\n",
    "        device=\"cpu\",\n",
    "    )\n",
    ")\n",
    "\n",
    "redacted_dataset = pii_modifier(pii_dataset)\n",
    "\n",
    "print(\"\\nPII ì œê±° í›„:\")\n",
    "display(redacted_dataset.df.compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ì£¼ì œ ë° ë¶€ì£¼ì œ ìƒì„±\n",
    "\n",
    "### 3.1 ë§¤í¬ë¡œ ì£¼ì œ ìƒì„± í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_macro_topics(\n",
    "    generator: NemotronGenerator,\n",
    "    model: str,\n",
    "    model_kwargs: dict,\n",
    "    n_macro_topics: int,\n",
    "    language: str = \"english\",\n",
    "    n_retries: int = 5,\n",
    ") -> List[str]:\n",
    "    \"\"\"ë§¤í¬ë¡œ ì£¼ì œ ìƒì„±\"\"\"\n",
    "    \n",
    "    # ì–¸ì–´ë³„ í”„ë¡¬í”„íŠ¸\n",
    "    prompts = {\n",
    "        \"english\": (\n",
    "            \"Can you generate {n_macro_topics} comprehensive topics that encompass \"\n",
    "            \"various aspects of our daily life, the world, and science? Your answer \"\n",
    "            \"should be a list of topics. Make the topics as diverse as possible. \"\n",
    "            \"For example, 1. Food and drinks. \\n2. Technology.\\n\"\n",
    "        ),\n",
    "        \"korean\": (\n",
    "            \"{n_macro_topics}ê°œì˜ í¬ê´„ì ì¸ ì£¼ì œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”. ì£¼ì œëŠ” ì¼ìƒìƒí™œ, ì„¸ê³„, \"\n",
    "            \"ê³¼í•™ì˜ ë‹¤ì–‘í•œ ì¸¡ë©´ì„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤. ë‹µë³€ì€ ì£¼ì œ ëª©ë¡ì´ì–´ì•¼ í•˜ë©°, \"\n",
    "            \"ê°€ëŠ¥í•œ í•œ ë‹¤ì–‘í•˜ê²Œ ë§Œë“¤ì–´ì£¼ì„¸ìš”. ì˜ˆ: 1. ìŒì‹ê³¼ ìŒë£Œ. \\n2. ê¸°ìˆ .\\n\"\n",
    "        ),\n",
    "    }\n",
    "    \n",
    "    prompt_template = prompts.get(language, prompts[\"english\"])\n",
    "    macro_topics = []\n",
    "    \n",
    "    for _ in range(n_retries):\n",
    "        try:\n",
    "            llm_response = generator.generate_macro_topics(\n",
    "                n_macro_topics=n_macro_topics,\n",
    "                model=model,\n",
    "                model_kwargs=model_kwargs,\n",
    "                prompt_template=prompt_template,\n",
    "            )\n",
    "            macro_topics = generator.convert_response_to_yaml_list(\n",
    "                llm_response=llm_response[0],\n",
    "                model=model,\n",
    "                model_kwargs=model_kwargs,\n",
    "            )\n",
    "            break\n",
    "        except YamlConversionError as e:\n",
    "            print(f\"ì¬ì‹œë„ ì¤‘... ({e})\")\n",
    "    \n",
    "    return macro_topics\n",
    "\n",
    "\n",
    "print(\"âœ… ë§¤í¬ë¡œ ì£¼ì œ ìƒì„± í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 ë¶€ì£¼ì œ ìƒì„± í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_subtopics(\n",
    "    generator: NemotronGenerator,\n",
    "    model: str,\n",
    "    model_kwargs: dict,\n",
    "    macro_topic: str,\n",
    "    n_subtopics: int,\n",
    "    language: str = \"english\",\n",
    "    n_retries: int = 5,\n",
    ") -> List[str]:\n",
    "    \"\"\"ë¶€ì£¼ì œ ìƒì„±\"\"\"\n",
    "    \n",
    "    prompts = {\n",
    "        \"english\": (\n",
    "            \"Generate {n_subtopics} topics that cover various aspects of {macro_topic}. \"\n",
    "            \"Your response should only be a list of topics, as diverse as possible. \"\n",
    "            \"Do not include explanations or additional text. For example: \"\n",
    "            \"1. Food and drinks. \\n2. Technology.\\n\"\n",
    "        ),\n",
    "        \"korean\": (\n",
    "            \"{macro_topic}ì˜ ë‹¤ì–‘í•œ ì¸¡ë©´ì„ ë‹¤ë£¨ëŠ” {n_subtopics}ê°œì˜ ì£¼ì œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”. \"\n",
    "            \"ë‹µë³€ì€ ê°€ëŠ¥í•œ í•œ ë‹¤ì–‘í•œ ì£¼ì œ ëª©ë¡ë§Œ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤. \"\n",
    "            \"ì„¤ëª…ì´ë‚˜ ì¶”ê°€ í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”. ì˜ˆ: 1. ìŒì‹ê³¼ ìŒë£Œ. \\n2. ê¸°ìˆ .\\n\"\n",
    "        ),\n",
    "    }\n",
    "    \n",
    "    prompt_template = prompts.get(language, prompts[\"english\"])\n",
    "    subtopics = []\n",
    "    \n",
    "    for _ in range(n_retries):\n",
    "        try:\n",
    "            llm_response = generator.generate_subtopics(\n",
    "                model=model,\n",
    "                model_kwargs=model_kwargs,\n",
    "                macro_topic=macro_topic,\n",
    "                n_subtopics=n_subtopics,\n",
    "                prompt_template=prompt_template,\n",
    "            )\n",
    "            subtopics = generator.convert_response_to_yaml_list(\n",
    "                llm_response=llm_response[0],\n",
    "                model=model,\n",
    "            )\n",
    "            break\n",
    "        except YamlConversionError as e:\n",
    "            print(f\"ì¬ì‹œë„ ì¤‘... ({e})\")\n",
    "    \n",
    "    return subtopics\n",
    "\n",
    "\n",
    "print(\"âœ… ë¶€ì£¼ì œ ìƒì„± í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 ì£¼ì œ ìƒì„± ì‹¤ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì˜ì–´ë¡œ ë§¤í¬ë¡œ ì£¼ì œ ìƒì„±\n",
    "print(\"ğŸ“ ë§¤í¬ë¡œ ì£¼ì œ ìƒì„± ì¤‘...\")\n",
    "macro_topics_en = generate_macro_topics(\n",
    "    generator=generator,\n",
    "    model=model,\n",
    "    model_kwargs=model_kwargs,\n",
    "    n_macro_topics=3,\n",
    "    language=\"english\",\n",
    ")\n",
    "\n",
    "print(\"\\nìƒì„±ëœ ë§¤í¬ë¡œ ì£¼ì œ (ì˜ì–´):\")\n",
    "for i, topic in enumerate(macro_topics_en, 1):\n",
    "    print(f\"{i}. {topic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì²« ë²ˆì§¸ ë§¤í¬ë¡œ ì£¼ì œì— ëŒ€í•œ ë¶€ì£¼ì œ ìƒì„±\n",
    "if macro_topics_en:\n",
    "    print(f\"\\nğŸ“ '{macro_topics_en[0]}'ì— ëŒ€í•œ ë¶€ì£¼ì œ ìƒì„± ì¤‘...\")\n",
    "    subtopics_en = generate_subtopics(\n",
    "        generator=generator,\n",
    "        model=model,\n",
    "        model_kwargs=model_kwargs,\n",
    "        macro_topic=macro_topics_en[0],\n",
    "        n_subtopics=3,\n",
    "        language=\"english\",\n",
    "    )\n",
    "    \n",
    "    print(\"\\nìƒì„±ëœ ë¶€ì£¼ì œ:\")\n",
    "    for i, subtopic in enumerate(subtopics_en, 1):\n",
    "        print(f\"  {i}. {subtopic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Q&A ë°ì´í„°ì…‹ ìƒì„±\n",
    "\n",
    "### 4.1 ì§ˆë¬¸ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions_from_topic(\n",
    "    generator: NemotronGenerator,\n",
    "    model: str,\n",
    "    model_kwargs: dict,\n",
    "    topic: str,\n",
    "    n_questions: int,\n",
    "    language: str = \"english\",\n",
    "    n_retries: int = 5,\n",
    ") -> List[str]:\n",
    "    \"\"\"ì£¼ì œì—ì„œ ì§ˆë¬¸ ìƒì„±\"\"\"\n",
    "    \n",
    "    prompts = {\n",
    "        \"english\": (\n",
    "            \"Can you generate {n_openlines} questions or requests related to {topic}? \"\n",
    "            \"The questions and requests should be as diverse as possible. \"\n",
    "            \"Your answer should be a list.\"\n",
    "        ),\n",
    "        \"korean\": (\n",
    "            \"{topic}ì™€ ê´€ë ¨ëœ {n_openlines}ê°œì˜ ì§ˆë¬¸ì´ë‚˜ ìš”ì²­ì„ ìƒì„±í•´ì£¼ì„¸ìš”. \"\n",
    "            \"ì§ˆë¬¸ê³¼ ìš”ì²­ì€ ê°€ëŠ¥í•œ í•œ ë‹¤ì–‘í•´ì•¼ í•©ë‹ˆë‹¤. ë‹µë³€ì€ ëª©ë¡ì´ì–´ì•¼ í•©ë‹ˆë‹¤.\"\n",
    "        ),\n",
    "    }\n",
    "    \n",
    "    prompt_template = prompts.get(language, prompts[\"english\"])\n",
    "    questions = []\n",
    "    \n",
    "    for _ in range(n_retries):\n",
    "        try:\n",
    "            llm_response = generator.generate_open_qa_from_topic(\n",
    "                model=model,\n",
    "                model_kwargs=model_kwargs,\n",
    "                topic=topic,\n",
    "                n_openlines=n_questions,\n",
    "                prompt_template=prompt_template,\n",
    "            )\n",
    "            questions = generator.convert_response_to_yaml_list(\n",
    "                llm_response=llm_response[0],\n",
    "                model=model,\n",
    "            )\n",
    "            break\n",
    "        except YamlConversionError as e:\n",
    "            print(f\"ì¬ì‹œë„ ì¤‘... ({e})\")\n",
    "    \n",
    "    return questions\n",
    "\n",
    "\n",
    "print(\"âœ… ì§ˆë¬¸ ìƒì„± í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 ë‹µë³€ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answers(\n",
    "    generator: NemotronGenerator,\n",
    "    user_model: str,\n",
    "    user_model_kwargs: dict,\n",
    "    assistant_model: str,\n",
    "    assistant_model_kwargs: dict,\n",
    "    questions: List[str],\n",
    "    n_retries: int = 5,\n",
    ") -> List[Dict[str, str]]:\n",
    "    \"\"\"ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±\"\"\"\n",
    "    \n",
    "    qa_pairs = []\n",
    "    \n",
    "    for question in questions:\n",
    "        for _ in range(n_retries):\n",
    "            try:\n",
    "                dialogue = generator.generate_dialogue(\n",
    "                    openline=question,\n",
    "                    user_model=user_model,\n",
    "                    user_model_kwargs=user_model_kwargs,\n",
    "                    assistant_model=assistant_model,\n",
    "                    assistant_model_kwargs=assistant_model_kwargs,\n",
    "                    n_user_turns=1,\n",
    "                )\n",
    "                \n",
    "                if len(dialogue) == 2 and \"content\" in dialogue[1]:\n",
    "                    answer = dialogue[1][\"content\"]\n",
    "                    if answer.startswith(\"1. \"):\n",
    "                        answer = answer[3:]\n",
    "                    \n",
    "                    qa_pairs.append({\n",
    "                        \"question\": question,\n",
    "                        \"answer\": answer\n",
    "                    })\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                print(f\"ì¬ì‹œë„ ì¤‘... ({e})\")\n",
    "    \n",
    "    return qa_pairs\n",
    "\n",
    "\n",
    "print(\"âœ… ë‹µë³€ ìƒì„± í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Reward Modelë¡œ í‰ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_qa_pairs(\n",
    "    qa_pairs: List[Dict[str, str]],\n",
    "    min_threshold: float = -20.0,\n",
    ") -> Tuple[List[Dict[str, str]], List[Dict[str, str]]]:\n",
    "    \"\"\"Reward Modelë¡œ Q&A ìŒ í‰ê°€\"\"\"\n",
    "    \n",
    "    retained = []\n",
    "    discarded = []\n",
    "    \n",
    "    for i, pair in enumerate(qa_pairs):\n",
    "        try:\n",
    "            messages = [\n",
    "                {\"role\": \"user\", \"content\": pair[\"question\"]},\n",
    "                {\"role\": \"assistant\", \"content\": pair[\"answer\"]}\n",
    "            ]\n",
    "            \n",
    "            response = openai_client.chat.completions.create(\n",
    "                model=\"nvidia/llama-3.1-nemotron-70b-reward\",\n",
    "                messages=messages,\n",
    "            )\n",
    "            \n",
    "            reward = float(response.choices[0].message.content.split(\":\")[1])\n",
    "            \n",
    "            if reward >= min_threshold:\n",
    "                print(f\"âœ… Q&A {i+1}: ì ìˆ˜ {reward:.2f} - ì±„íƒ\")\n",
    "                retained.append(pair)\n",
    "            else:\n",
    "                print(f\"âŒ Q&A {i+1}: ì ìˆ˜ {reward:.2f} - ì œì™¸\")\n",
    "                discarded.append(pair)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸  Q&A {i+1} í‰ê°€ ì˜¤ë¥˜: {e}\")\n",
    "    \n",
    "    return retained, discarded\n",
    "\n",
    "\n",
    "print(\"âœ… í‰ê°€ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Q&A ìƒì„± ì‹¤ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì£¼ì œ ì„¤ì •\n",
    "topic = \"Artificial Intelligence and Machine Learning\"\n",
    "\n",
    "# ì§ˆë¬¸ ìƒì„±\n",
    "print(f\"ğŸ“ '{topic}'ì— ëŒ€í•œ ì§ˆë¬¸ ìƒì„± ì¤‘...\")\n",
    "questions = generate_questions_from_topic(\n",
    "    generator=generator,\n",
    "    model=model,\n",
    "    model_kwargs=model_kwargs,\n",
    "    topic=topic,\n",
    "    n_questions=2,\n",
    "    language=\"english\",\n",
    ")\n",
    "\n",
    "print(\"\\nìƒì„±ëœ ì§ˆë¬¸:\")\n",
    "for i, q in enumerate(questions, 1):\n",
    "    print(f\"{i}. {q}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‹µë³€ ìƒì„±\n",
    "if questions:\n",
    "    print(\"\\nğŸ“ ë‹µë³€ ìƒì„± ì¤‘...\")\n",
    "    qa_pairs = generate_answers(\n",
    "        generator=generator,\n",
    "        user_model=model,\n",
    "        user_model_kwargs=model_kwargs,\n",
    "        assistant_model=model,\n",
    "        assistant_model_kwargs=model_kwargs,\n",
    "        questions=questions,\n",
    "    )\n",
    "    \n",
    "    print(\"\\nìƒì„±ëœ Q&A:\")\n",
    "    for i, pair in enumerate(qa_pairs, 1):\n",
    "        print(f\"\\n[Q{i}] {pair['question']}\")\n",
    "        print(f\"[A{i}] {pair['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward Modelë¡œ í‰ê°€\n",
    "if qa_pairs:\n",
    "    print(\"\\nğŸ“Š í’ˆì§ˆ í‰ê°€ ì¤‘...\")\n",
    "    retained, discarded = evaluate_qa_pairs(qa_pairs, min_threshold=-20.0)\n",
    "    \n",
    "    print(f\"\\ní‰ê°€ ê²°ê³¼: ì±„íƒ {len(retained)}ê°œ, ì œì™¸ {len(discarded)}ê°œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. ìˆ˜í•™ ë¬¸ì œ ìƒì„±\n",
    "\n",
    "### 5.1 ìˆ˜í•™ ë¬¸ì œ ìƒì„± í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_math_response(llm_response: str) -> List[Tuple[str, str]]:\n",
    "    \"\"\"ìˆ˜í•™ ë¬¸ì œ ì‘ë‹µ íŒŒì‹±\"\"\"\n",
    "    pattern = r\">>>Problem<<<:(.*?)>>>Solution<<<:(.*?)(?=\\(\\(###|\\Z)\"\n",
    "    matches = re.findall(pattern, llm_response, re.DOTALL)\n",
    "    \n",
    "    problems = []\n",
    "    for problem, solution in matches:\n",
    "        problems.append((\n",
    "            problem.strip().replace(\">>>Problem<<<:\", \"\").strip(),\n",
    "            solution.strip().replace(\">>>Solution<<<:\", \"\").strip()\n",
    "        ))\n",
    "    \n",
    "    return problems\n",
    "\n",
    "\n",
    "def generate_math_problems(\n",
    "    generator: NemotronGenerator,\n",
    "    model: str,\n",
    "    model_kwargs: dict,\n",
    "    topic: str,\n",
    "    n_problems: int,\n",
    "    language: str = \"english\",\n",
    "    n_retries: int = 5,\n",
    ") -> List[Tuple[str, str]]:\n",
    "    \"\"\"ìˆ˜í•™ ë¬¸ì œ ìƒì„±\"\"\"\n",
    "    \n",
    "    prompts = {\n",
    "        \"english\": (\n",
    "            \"Create {n_openlines} diverse mathematics problems related to the topic '{topic}' \"\n",
    "            \"or solvable using concepts from '{topic}'. Provide your response as a numbered list, \"\n",
    "            \"and include both the problem description and its solution. Format your response as follows: \"\n",
    "            \"((###1###)) >>>Problem<<<: [Description of the first problem]. >>>Solution<<<: [Solution to the first problem].\\n\"\n",
    "            \"Only include the problems and their solutionsâ€”no additional text.\"\n",
    "        ),\n",
    "        \"korean\": (\n",
    "            \"'{topic}' ì£¼ì œì™€ ê´€ë ¨ë˜ê±°ë‚˜ '{topic}' ê°œë…ì„ ì‚¬ìš©í•˜ì—¬ í’€ ìˆ˜ ìˆëŠ” {n_openlines}ê°œì˜ \"\n",
    "            \"ë‹¤ì–‘í•œ ìˆ˜í•™ ë¬¸ì œë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”. ë²ˆí˜¸ê°€ ë§¤ê²¨ì§„ ëª©ë¡ìœ¼ë¡œ ë‹µë³€ì„ ì œê³µí•˜ê³ , \"\n",
    "            \"ë¬¸ì œ ì„¤ëª…ê³¼ í•´ë‹µì„ ëª¨ë‘ í¬í•¨í•˜ì„¸ìš”. ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”: \"\n",
    "            \"((###1###)) >>>ë¬¸ì œ<<<: [ì²« ë²ˆì§¸ ë¬¸ì œ ì„¤ëª…]. >>>í•´ë‹µ<<<: [ì²« ë²ˆì§¸ ë¬¸ì œì˜ í•´ë‹µ].\\n\"\n",
    "            \"ë¬¸ì œì™€ í•´ë‹µë§Œ í¬í•¨í•˜ê³  ì¶”ê°€ í…ìŠ¤íŠ¸ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.\"\n",
    "        ),\n",
    "    }\n",
    "    \n",
    "    prompt_template = prompts.get(language, prompts[\"english\"])\n",
    "    math_problems = []\n",
    "    \n",
    "    for _ in range(n_retries):\n",
    "        try:\n",
    "            llm_response = generator.generate_math_problem(\n",
    "                model=model,\n",
    "                topic=topic,\n",
    "                n_openlines=n_problems,\n",
    "                prompt_template=prompt_template,\n",
    "            )\n",
    "            math_problems = parse_math_response(llm_response[0])\n",
    "            if math_problems:\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"ì¬ì‹œë„ ì¤‘... ({e})\")\n",
    "    \n",
    "    return math_problems\n",
    "\n",
    "\n",
    "print(\"âœ… ìˆ˜í•™ ë¬¸ì œ ìƒì„± í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 ìˆ˜í•™ ë¬¸ì œ ìƒì„± ì‹¤ìŠµ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìˆ˜í•™ ì£¼ì œ ì„¤ì •\n",
    "math_topic = \"Algebra\"\n",
    "\n",
    "print(f\"ğŸ“ '{math_topic}'ì— ëŒ€í•œ ìˆ˜í•™ ë¬¸ì œ ìƒì„± ì¤‘...\")\n",
    "math_problems = generate_math_problems(\n",
    "    generator=generator,\n",
    "    model=model,\n",
    "    model_kwargs=model_kwargs,\n",
    "    topic=math_topic,\n",
    "    n_problems=2,\n",
    "    language=\"english\",\n",
    ")\n",
    "\n",
    "print(\"\\nìƒì„±ëœ ìˆ˜í•™ ë¬¸ì œ:\")\n",
    "for i, (problem, solution) in enumerate(math_problems, 1):\n",
    "    print(f\"\\n[ë¬¸ì œ {i}]\")\n",
    "    print(f\"Problem: {problem}\")\n",
    "    print(f\"Solution: {solution}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ì¢…í•© ì‹¤ìŠµ\n",
    "\n",
    "### 6.1 ì „ì²´ íŒŒì´í”„ë¼ì¸ í†µí•©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_data_curation_pipeline(\n",
    "    generator: NemotronGenerator,\n",
    "    model: str,\n",
    "    model_kwargs: dict,\n",
    "    language: str = \"english\",\n",
    "    n_macro_topics: int = 2,\n",
    "    n_subtopics: int = 2,\n",
    "    n_questions_per_subtopic: int = 2,\n",
    "    n_math_problems: int = 2,\n",
    ") -> Dict:\n",
    "    \"\"\"ì „ì²´ ë°ì´í„° íë ˆì´ì…˜ íŒŒì´í”„ë¼ì¸\"\"\"\n",
    "    \n",
    "    results = {\n",
    "        \"macro_topics\": [],\n",
    "        \"subtopics\": {},\n",
    "        \"qa_pairs\": [],\n",
    "        \"math_problems\": [],\n",
    "    }\n",
    "    \n",
    "    # 1. ë§¤í¬ë¡œ ì£¼ì œ ìƒì„±\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"1ë‹¨ê³„: ë§¤í¬ë¡œ ì£¼ì œ ìƒì„±\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    macro_topics = generate_macro_topics(\n",
    "        generator, model, model_kwargs, n_macro_topics, language\n",
    "    )\n",
    "    results[\"macro_topics\"] = macro_topics\n",
    "    \n",
    "    for i, topic in enumerate(macro_topics, 1):\n",
    "        print(f\"{i}. {topic}\")\n",
    "    \n",
    "    # 2. ë¶€ì£¼ì œ ìƒì„±\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"2ë‹¨ê³„: ë¶€ì£¼ì œ ìƒì„±\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for macro_topic in macro_topics:\n",
    "        print(f\"\\nğŸ“Œ {macro_topic}\")\n",
    "        subtopics = generate_subtopics(\n",
    "            generator, model, model_kwargs, macro_topic, n_subtopics, language\n",
    "        )\n",
    "        results[\"subtopics\"][macro_topic] = subtopics\n",
    "        \n",
    "        for i, subtopic in enumerate(subtopics, 1):\n",
    "            print(f\"  {i}. {subtopic}\")\n",
    "    \n",
    "    # 3. Q&A ìƒì„± (ì²« ë²ˆì§¸ ë¶€ì£¼ì œë§Œ)\n",
    "    if macro_topics and results[\"subtopics\"]:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"3ë‹¨ê³„: Q&A ìƒì„±\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        first_macro = macro_topics[0]\n",
    "        first_subtopic = results[\"subtopics\"][first_macro][0]\n",
    "        \n",
    "        print(f\"\\nì£¼ì œ: {first_subtopic}\")\n",
    "        \n",
    "        questions = generate_questions_from_topic(\n",
    "            generator, model, model_kwargs, first_subtopic,\n",
    "            n_questions_per_subtopic, language\n",
    "        )\n",
    "        \n",
    "        qa_pairs = generate_answers(\n",
    "            generator, model, model_kwargs, model, model_kwargs, questions\n",
    "        )\n",
    "        \n",
    "        results[\"qa_pairs\"] = qa_pairs\n",
    "        \n",
    "        for i, pair in enumerate(qa_pairs, 1):\n",
    "            print(f\"\\n[Q{i}] {pair['question']}\")\n",
    "            print(f\"[A{i}] {pair['answer'][:100]}...\")\n",
    "    \n",
    "    # 4. ìˆ˜í•™ ë¬¸ì œ ìƒì„±\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"4ë‹¨ê³„: ìˆ˜í•™ ë¬¸ì œ ìƒì„±\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    math_topic = \"Mathematics\" if language == \"english\" else \"ìˆ˜í•™\"\n",
    "    math_problems = generate_math_problems(\n",
    "        generator, model, model_kwargs, math_topic, n_math_problems, language\n",
    "    )\n",
    "    \n",
    "    results[\"math_problems\"] = math_problems\n",
    "    \n",
    "    for i, (problem, solution) in enumerate(math_problems, 1):\n",
    "        print(f\"\\n[ë¬¸ì œ {i}] {problem[:100]}...\")\n",
    "        print(f\"[í•´ë‹µ {i}] {solution[:100]}...\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"âœ… í†µí•© íŒŒì´í”„ë¼ì¸ í•¨ìˆ˜ ì •ì˜ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰\n",
    "print(\"ğŸš€ ì „ì²´ ë°ì´í„° íë ˆì´ì…˜ íŒŒì´í”„ë¼ì¸ ì‹œì‘\")\n",
    "print(\"ì´ ì‘ì—…ì€ ëª‡ ë¶„ ì •ë„ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤...\\n\")\n",
    "\n",
    "results = complete_data_curation_pipeline(\n",
    "    generator=generator,\n",
    "    model=model,\n",
    "    model_kwargs=model_kwargs,\n",
    "    language=\"english\",\n",
    "    n_macro_topics=2,\n",
    "    n_subtopics=2,\n",
    "    n_questions_per_subtopic=2,\n",
    "    n_math_problems=2,\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 ê²°ê³¼ ìš”ì•½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²°ê³¼ ìš”ì•½\n",
    "print(\"\\nğŸ“Š ìƒì„±ëœ ë°ì´í„° ìš”ì•½\")\n",
    "print(\"=\"*80)\n",
    "print(f\"ë§¤í¬ë¡œ ì£¼ì œ: {len(results['macro_topics'])}ê°œ\")\n",
    "print(f\"ë¶€ì£¼ì œ: {sum(len(subs) for subs in results['subtopics'].values())}ê°œ\")\n",
    "print(f\"Q&A ìŒ: {len(results['qa_pairs'])}ê°œ\")\n",
    "print(f\"ìˆ˜í•™ ë¬¸ì œ: {len(results['math_problems'])}ê°œ\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 ê²°ê³¼ ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥\n",
    "output_dir = \"./output\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_file = os.path.join(output_dir, f\"curation_results_{timestamp}.json\")\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\nğŸ’¾ ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ë§ˆë¬´ë¦¬\n",
    "\n",
    "### í•™ìŠµ ë‚´ìš© ì •ë¦¬\n",
    "\n",
    "ì´ ë…¸íŠ¸ë¶ì„ í†µí•´ ë‹¤ìŒì„ í•™ìŠµí–ˆìŠµë‹ˆë‹¤:\n",
    "\n",
    "1. **ê¸°ë³¸ ë°ì´í„° íë ˆì´ì…˜**\n",
    "   - í…ìŠ¤íŠ¸ ì •ë¦¬ ë° ì •ê·œí™”\n",
    "   - ë¬¸ì„œ í•„í„°ë§\n",
    "   - PII ì œê±°\n",
    "\n",
    "2. **í•©ì„± ë°ì´í„° ìƒì„±**\n",
    "   - ë§¤í¬ë¡œ ì£¼ì œ ë° ë¶€ì£¼ì œ ìƒì„±\n",
    "   - Q&A ë°ì´í„°ì…‹ ìƒì„±\n",
    "   - ìˆ˜í•™ ë¬¸ì œ ìƒì„±\n",
    "\n",
    "3. **í’ˆì§ˆ í‰ê°€**\n",
    "   - Reward Modelì„ í™œìš©í•œ í’ˆì§ˆ í‰ê°€\n",
    "   - í’ˆì§ˆ ê¸°ë°˜ í•„í„°ë§\n",
    "\n",
    "4. **í†µí•© íŒŒì´í”„ë¼ì¸**\n",
    "   - ì „ì²´ ì›Œí¬í”Œë¡œìš° ìë™í™”\n",
    "   - ê²°ê³¼ ì €ì¥ ë° ê´€ë¦¬\n",
    "\n",
    "### ë‹¤ìŒ ë‹¨ê³„\n",
    "\n",
    "- ê°œë³„ ë…¸íŠ¸ë¶ (`01_basics_curation.ipynb` ë“±)ì„ í†µí•´ ê° ì£¼ì œë¥¼ ë” ê¹Šì´ í•™ìŠµ\n",
    "- ë‹¤ì–‘í•œ ì–¸ì–´ì™€ ë„ë©”ì¸ìœ¼ë¡œ ì‹¤í—˜\n",
    "- ìƒì„±ëœ ë°ì´í„°ë¡œ ëª¨ë¸ ë¯¸ì„¸ ì¡°ì •\n",
    "\n",
    "### í´ëŸ¬ìŠ¤í„° ì¢…ë£Œ\n",
    "\n",
    "ì‘ì—…ì„ ë§ˆì¹˜ë©´ Dask í´ëŸ¬ìŠ¤í„°ë¥¼ ì¢…ë£Œí•˜ì„¸ìš”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dask í´ëŸ¬ìŠ¤í„° ì¢…ë£Œ\n",
    "client.close()\n",
    "print(\"âœ… Dask í´ëŸ¬ìŠ¤í„°ê°€ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
